[
  {
    "id": "azure-databricks",
    "title": "Azure Databricks",
    "sections": [
      {
        "title": "Introduction to Azure Databricks",
        "content": [
          {
            "type": "heading",
            "text": "What is Azure Databricks?",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Azure Databricks is a cloud platform that helps people work with data and artificial intelligence in one place. It brings together tools for data engineering, data science, and machine learning, so teams can easily collect, clean, and analyze data.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It uses a ‚Äúlakehouse‚Äù design, a mix of a data lake and a data warehouse, which makes it easier to store large amounts of data and use it quickly for insights or AI projects. Databricks is built on open-source tools like Apache Spark and Delta Lake, and it can run on major cloud platforms such as Azure, AWS, and Google Cloud.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Azure Databricks is a cloud-based platform that helps you work with data, analytics, and AI in one place. It combines tools for storing, processing, and analyzing data so that teams can easily build and share data projects.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It connects directly with your cloud storage and takes care of setting up and managing the required infrastructure for you.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Using Generative AI, Azure Databricks can understand your data and automatically improve performance to meet your needs. It also uses natural language processing (NLP), which means you can find data or get help just by typing questions in plain English. It can even help you write code, fix issues, and explore documentation easily.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Features of Azure Databricks",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Unified Workspace:A single place where data engineers, data scientists, and analysts can work together on data and AI projects.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Lakehouse Architecture:Combines the best parts of data lakes and data warehouses, making it easier to store and use data efficiently.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Scalability:Automatically adjusts resources based on your workload, so you can handle small or large amounts of data easily.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Built on Apache Spark:Uses Spark, a fast and powerful open-source engine, to process large data quickly.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Delta Lake Integration:Ensures your data is reliable and consistent by handling updates and corrections efficiently.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Collaborative Notebooks:Let's teams write code, visualize data, and share work in real time using notebooks that support Python, SQL, R, and Scala.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Seamless Cloud Integration:Works smoothly with Azure services like Data Lake Storage, Synapse, Machine Learning, and Power BI.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "AI and Machine Learning Support:Provides built-in tools to train, test, and deploy machine learning and AI models easily.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Security and Compliance:Protects your data using Azure‚Äôs enterprise-grade security, including encryption, role-based access, and compliance certifications.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Natural Language Assistance (Generative AI):Let's users find data, write code, and fix errors just by asking questions in plain English.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Databricks Architecture",
        "content": [
          {
            "type": "paragraph",
            "text": "Azure Databricks follows a multi-layer architecture built on top of Apache Spark and Delta Lake, integrated deeply with Azure cloud services. It unifies data engineering, analytics, and AI within a single environment.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Storage Layer (Data and Delta Lake):",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Connects directly to cloud storage such as Azure Data Lake Storage (ADLS Gen2) or Blob Storage.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Delta Lake acts as the transactional storage layer, providing:",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "ACID compliance",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Schema enforcement and evolution",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Data versioning (time travel)",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Scalable metadata handling",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Compute Layer (Clusters and Runtime):",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Uses Databricks Clusters ‚Äî groups of VMs ‚Äî for distributed data processing.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Powered by Databricks Runtime (DBR), an optimized engine based on Apache Spark.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Supports autoscaling, auto-termination, and GPU/CPU clusters for different workloads.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Control Plane:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Managed by Databricks (in Azure).",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Handles user authentication, workspace management, notebook storage, job scheduling, and cluster configuration.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Stores metadata and notebook information securely.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Plane:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Runs inside your Azure subscription.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Responsible for actual data processing and storage.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "All data remains in your cloud environment ‚Äî ensuring compliance and security.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Workspace / User Interface Layer:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A collaborative web-based environment for developers, data engineers, and scientists.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Supports multiple languages ‚Äî Python, SQL, R, Scala, Java.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Includes features like notebooks, repos, dashboards, and job orchestration.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Machine Learning and AI Layer:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Integrates MLflow for experiment tracking, model registry, and deployment.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Supports integration with Azure Machine Learning for end-to-end MLOps.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Security and Governance Layer:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Managed through Unity Catalog for centralized access control, data lineage, and auditing.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Uses Azure Active Directory (AAD) for authentication and RBAC for authorization.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Common Use Cases of Azure Databricks",
        "content": [
          {
            "type": "heading",
            "text": "Data Engineering:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Used to collect, clean, and prepare large amounts of data from different sources before analysis or reporting.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Analytics:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Helps analyze and visualize data to find useful patterns and trends for better decision-making.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Machine Learning and AI:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Allows users to train, test, and deploy machine learning and AI models directly within the platform.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Real-Time Data Processing:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Can handle streaming data ‚Äî for example, analyzing live sensor data or real-time transactions.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Warehousing and BI:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Works with tools like Power BI to create reports and dashboards from stored data.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "ETL (Extract, Transform, Load) Pipelines:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Automates the process of moving and transforming data from one system to another for analysis.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Lakehouse Management:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Combines data lake storage with data warehouse features, making it easier to manage both structured and unstructured data.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Collaborative Data Projects:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Let‚Äôs teams of data engineers and data scientists work together in shared notebooks and environments.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Predictive Analytics:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Used to forecast trends or outcomes ‚Äî for example, predicting customer behaviour, sales, or equipment failure.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Core Components of Azure Databricks",
        "content": [
          {
            "type": "heading",
            "text": "Workspace:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "This is the main area where you and your team can create notebooks, manage data, and work together on data and AI projects.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Notebooks:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Interactive notebooks where you can write and run code in languages like Python, SQL, R, or Scala to explore and visualize data.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Clusters:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Groups of virtual machines that run your data processing tasks. They automatically scale up or down based on the workload.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Jobs:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Used to schedule and automate tasks like data processing, transformations, or machine learning model training.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Lake and Delta Lake:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Delta Lake stores and manages your data in a reliable way, adding features like version control, updates, and rollbacks on top of your data lake.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Databricks Runtime:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The engine that runs your Spark jobs ‚Äî it‚Äôs optimized for faster performance and lower costs.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Repos (Version Control):",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Lets you connect GitHub or Azure DevOps for source control, so you can manage and track changes to your code easily.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "MLflow:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A built-in tool for managing the complete machine learning lifecycle ‚Äî from model training and tracking to deployment.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Unity Catalog:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A centralized data governance and access management system that helps control who can access which data across the platform.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Dashboarding and Visualization:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Allows you to create charts, graphs, and dashboards to share insights and monitor your data pipelines.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Advantages of Azure Databricks",
        "content": [
          {
            "type": "heading",
            "text": "Unified Analytics and AI Platform:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Combines data engineering, data science, and analytics into a single, collaborative workspace for end-to-end data workflows.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "High Performance and Scalability:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Optimized Apache Spark runtime ensures faster execution, while autoscaling dynamically adjusts cluster size to handle any workload efficiently.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Delta Lake Reliability:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Provides ACID transactions, schema enforcement, and time travel features for consistent and reliable data pipelines.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Deep Azure Ecosystem Integration:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Natively connects with Azure Data Lake Storage, Synapse Analytics, Power BI, Azure ML, and Active Directory for seamless interoperability.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Multi-Language and Multi-User Collaboration:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Supports Python, SQL, R, Scala, and Java within shared notebooks for cross-functional team collaboration.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Automated Cluster and Job Management:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Simplifies operational overhead with autoscaling, auto-termination, and job scheduling capabilities.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Advanced Security and Governance:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Offers enterprise-grade security through RBAC, encryption at rest/in transit, and governance via Unity Catalog.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Integrated ML and MLOps:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Built-in MLflow enables experiment tracking, model versioning, and deployment supporting the full ML lifecycle.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Cost Optimization:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Pay-as-you-go model with efficient resource utilization and intelligent scaling reduces infrastructure costs.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "AI-Powered Assistance:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Integrates generative AI and natural language capabilities for faster code generation, data discovery, and troubleshooting.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Databricks",
        "content": []
      },
      {
        "title": "How to Create Azure Databricks",
        "content": [
          {
            "type": "list",
            "items": [
              "Go to the Azure portal and search for Databricks"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-01.png",
            "alt": "Image 1"
          },
          {
            "type": "list",
            "items": [
              "Click on create"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-02.png",
            "alt": "Image 2"
          },
          {
            "type": "list",
            "items": [
              "Create databricks"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-03.png",
            "alt": "Image 3"
          },
          {
            "type": "heading",
            "text": "Subscription",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Choose the Azure subscription under which the Databricks workspace will be created.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Example: Azure subscription 1",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Resource Group",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Select an existing Resource Group or create a new one.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Resource groups act like folders to organize and manage related resources.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Example: rg-ohg365-dev",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Workspace Name",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Enter a unique workspace name for your Databricks instance.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Example: ohg365-db-dev",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Region",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Choose the Azure region where your workspace will be hosted.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Example: Central US",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Pricing Tier",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Select the pricing tier ‚Äî typically Premium (+ Role-based access controls) for better management and security features.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Managed Resource Group Name",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Azure automatically creates a Managed Resource Group to hold internal resources required by Databricks.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Example: mg-ohg365-db-dev",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Final Step ‚Äì Review + Create",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Click Review + create to validate your settings and proceed with workspace creation.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-04.png",
            "alt": "Image 4"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-05.png",
            "alt": "Image 5"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-06.png",
            "alt": "Image 6"
          },
          {
            "type": "paragraph",
            "text": "While creating an Azure Databricks workspace, Azure automatically creates a separate resource group called a Managed Resource Group. This group contains and manages all the supporting resources required for the Databricks workspace, as shown in the screenshot below.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-07.png",
            "alt": "Image 7"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-08.png",
            "alt": "Image 8"
          }
        ]
      },
      {
        "title": "Databricks Workspace Overview",
        "content": [
          {
            "type": "paragraph",
            "text": "Click on Databricks Workspace",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-09.png",
            "alt": "Image 9"
          },
          {
            "type": "paragraph",
            "text": "Click on the lunch workspace button",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-10.png",
            "alt": "Image 10"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-11.png",
            "alt": "Image 11"
          },
          {
            "type": "heading",
            "text": "Left Sidebar (Navigation Menu):",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The left-hand menu provides quick access to all major Databricks features and tools:",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Workspace: Where you can create and organize notebooks, folders, and projects.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Recents: Shows recently opened notebooks or files.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Catalog: Central place to access and manage data using Unity Catalog. It is delta lake.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Jobs & Pipelines: For automating workflows, scheduling data processing, or running ETL pipelines.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Compute: Manage clusters and compute resources used for data processing.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Marketplace: Discover and use prebuilt datasets, notebooks, and solutions.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "SQL Section:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "SQL Editor: Write and run SQL queries.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Queries / Dashboards: Create and view reports and dashboards.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Genie & Alerts: Access AI-powered query tools and set up notifications.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "SQL Warehouses: Manage dedicated SQL compute environments.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Engineering Section:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Job Runs / Data Ingestion: Monitor job executions and load data into Databricks.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "AI/ML Section:",
            "heading_level": 2
          },
          {
            "type": "heading",
            "text": "üîπ 1. Playground (Mosaic AI Playground)",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "The Playground is an interactive environment that allows you to experiment with generative AI models (like LLMs).",
              "You can test prompts, analyze responses, and refine model behavior ‚Äî all in a no-code or low-code interface.",
              "It‚Äôs designed to help you quickly prototype AI use cases such as:",
              "Chatbots",
              "Text summarization",
              "Sentiment analysis",
              "Document Q&A systems",
              "Uses Mosaic AI technology to connect your own business data with foundation models securely."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 2. Experiments",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "The Experiments feature helps you track, compare, and manage machine learning runs.",
              "It integrates with MLflow Tracking, allowing you to:",
              "Record parameters, metrics, and model versions.",
              "Compare results of multiple training runs.",
              "Identify which model performed best."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 3. Features (Feature Store)",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "The Feature Store is a central repository for machine learning features.",
              "It allows teams to create, share, and reuse features across multiple models and projects.",
              "Key benefits:",
              "Avoid duplication of feature engineering work.",
              "Maintain consistency between training and serving data.",
              "Improve model accuracy and governance."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 4. Models (Model Registry)",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "The Model Registry is where you store, version, and manage ML models created during experiments.",
              "You can:",
              "Track model versions and metadata.",
              "Approve or reject models for production.",
              "Manage model lifecycle stages: Staging ‚Üí Production ‚Üí Archived.",
              "Fully integrated with MLflow, ensuring seamless collaboration between teams."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 5. Serving (Model Serving)",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Model Serving lets you deploy ML models as REST API endpoints directly from Databricks.",
              "Supports real-time and batch predictions.",
              "Automatically scales based on usage and integrates with your data pipelines.",
              "Allows secure access to deployed models with minimal setup."
            ]
          },
          {
            "type": "heading",
            "text": "Main Panel (Welcome Screen):",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Displays a welcome message and a quick setup option ‚Äî ‚ÄúSet up your workspace.‚Äù",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Provides a search bar to quickly find data, notebooks, or past work.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Contains quick links like Recents, Favorites, Popular, and Mosaic AI to navigate faster.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "The ‚Äú+ New‚Äù button lets you start creating a new notebook, job, or dashboard immediately.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Top Navigation Bar:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Shows your workspace name (e.g., ohg365-db-dev).",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Allows switching between workspaces or accessing your account settings.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Contains shortcuts to Microsoft Azure and Databricks home.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Databricks Features",
        "content": [
          {
            "type": "heading",
            "text": "Workspace",
            "heading_level": 2
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-12.png",
            "alt": "Image 12"
          },
          {
            "type": "paragraph",
            "text": "The Workspace in Databricks is a collaborative environment where data engineers, data scientists, and analysts can create, share, and manage all Databricks-related resources such as notebooks, libraries, dashboards, and folders.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ Key Components in the Workspace",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Component",
                "Description"
              ],
              [
                "Repos",
                "Used for Git integration. It allows you to link your Databricks workspace to repositories in GitHub, Azure DevOps, or Bitbucket to manage version control for notebooks and projects."
              ],
              [
                "Shared",
                "A shared folder accessible to multiple team members in your workspace. It‚Äôs commonly used for collaboration on notebooks, models, and scripts."
              ],
              [
                "Users",
                "Contains individual user folders. Each user has a personal workspace where they can create and manage private notebooks and experiments."
              ],
              [
                "Home / Shared with me",
                "‚ÄúHome‚Äù is your personal starting directory, while ‚ÄúShared with me‚Äù lists notebooks or folders shared by other users."
              ],
              [
                "Favorites / Trash",
                "- Favorites: Quickly access important or frequently used notebooks.- Trash: Contains deleted notebooks or folders which can be restored or permanently removed."
              ]
            ]
          },
          {
            "type": "paragraph",
            "text": "Shared: Shared with me lists notebooks or folders shared by other users.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-13.png",
            "alt": "Image 13"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-14.png",
            "alt": "Image 14"
          },
          {
            "type": "paragraph",
            "text": "Users: Contains individual user folders. Each user has a personal workspace where they can create and manage private notebooks and experiments.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-15.png",
            "alt": "Image 15"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-16.png",
            "alt": "Image 16"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-17.png",
            "alt": "Image 17"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-18.png",
            "alt": "Image 18"
          },
          {
            "type": "heading",
            "text": "Other Creation Options in the Dropdown",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Purpose"
              ],
              [
                "Folder",
                "Create a new folder to organize notebooks or scripts."
              ],
              [
                "Git Folder",
                "Connect to a Git repository for version control."
              ],
              [
                "Notebook",
                "Create a new Databricks notebook for code, visualization, or data analysis (Python, SQL, R, or Scala)."
              ],
              [
                "File",
                "Upload or create a script or configuration file."
              ],
              [
                "Query",
                "Write SQL queries directly against your datasets."
              ],
              [
                "Dashboard",
                "Build visual dashboards from your queries."
              ],
              [
                "Genie Space",
                "Access AI-powered analytics assistant."
              ],
              [
                "ETL Pipeline",
                "Design and automate data pipelines."
              ],
              [
                "Alert",
                "Set up notifications for query results or data changes."
              ],
              [
                "MLflow Experiment",
                "Track machine learning experiments, metrics, and models."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Notebook:",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Azure Databricks notebooks serve as a collaborative development environment for building data science, engineering, and machine learning workflows.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "They support multi-language scripting within a single document, real-time coauthoring, version control, and integrated data visualization.These features help streamline code development, data exploration, and result presentation in a unified platform.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Create First Notebook:",
            "heading_level": 2
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-19.png",
            "alt": "Image 19"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-20.png",
            "alt": "Image 20"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-21.png",
            "alt": "Image 21"
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-22.png",
            "alt": "Image 22"
          },
          {
            "type": "paragraph",
            "text": "Recent‚Äôs: Shows recently opened notebooks or files.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-23.png",
            "alt": "Image 23"
          },
          {
            "type": "heading",
            "text": "Catalog and Features (Unity Catalog)",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Catalog in Azure Databricks is a central place to organize, manage, and secure all your data assets such as databases, tables, views, and files ‚Äî across your entire Databricks environment.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It provides data governance, access control, and data discovery in one interface.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-24.png",
            "alt": "Image 24"
          },
          {
            "type": "heading",
            "text": "Key Components in the Screenshot",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Section",
                "Description"
              ],
              [
                "My Organization",
                "Lists catalogs created within your workspace ‚Äî for example, ohg365_db_dev, system, and others. These hold schemas (databases) and tables."
              ],
              [
                "Delta Shares Received",
                "Displays data shared with you from other Databricks workspaces using Delta Sharing, a secure open protocol for data sharing."
              ],
              [
                "Legacy (hive_metastore)",
                "The old default data catalog (used before Unity Catalog). It contains older Hive-based tables and schemas."
              ],
              [
                "Search Bar",
                "Lets you quickly find data assets (catalogs, schemas, tables)."
              ],
              [
                "Quick Access (Right Panel)",
                "Displays recently viewed or favorite datasets, making it easier to return to frequently used data."
              ],
              [
                "Top Menu Options:",
                ""
              ],
              [
                "- Delta Sharing",
                "Manage secure data sharing between organizations."
              ],
              [
                "- Clean Rooms",
                "Enable collaboration on shared data without moving or copying it."
              ],
              [
                "- External Data",
                "Connect to external sources like Azure Data Lake or Blob Storage."
              ],
              [
                "- Governance",
                "Manage access permissions, auditing, and compliance."
              ],
              [
                "- Add Data",
                "Option to import or register new datasets into the catalog."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Jobs & Pipelines",
            "heading_level": 2
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-25.png",
            "alt": "Image 25"
          },
          {
            "type": "paragraph",
            "text": "The Jobs & Pipelines interface in Azure Databricks provides a unified orchestration layer for data engineering and machine learning workflows.It supports job scheduling, dependency management, pipeline orchestration, and execution monitoring.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Features",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Ingestion Pipelines: Automate ingestion from external data sources (databases, APIs, or files).",
              "ETL Pipelines: Design scalable, production-grade ETL processes using SQL, PySpark, or Python.",
              "Jobs: Orchestrate notebooks, workflows, pipelines, and queries; configure parameters, cluster settings, and triggers.",
              "Job Runs Dashboard: Monitor run history, logs, and metrics for troubleshooting and optimization.",
              "Access Control: Manage visibility (‚ÄúOwned by me,‚Äù ‚ÄúAccessible by me‚Äù) to enforce workspace-level governance."
            ]
          },
          {
            "type": "heading",
            "text": "Use Case",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Used by data engineers and ML teams to build end-to-end pipelines from data ingestion to transformation, feature generation, and model retraining all under one environment.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-26.png",
            "alt": "Image 26"
          },
          {
            "type": "paragraph",
            "text": "The Job Runs dashboard in Databricks provides an operational view of scheduled or triggered workflows.It allows engineers and ML teams to monitor, debug, and analyze job executions across environments.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Functionalities",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Run Filtering: Filter runs by job, user, time range, run status, or error code.",
              "Run Visualization: Graph at the top visualizes the number of successful, failed, or skipped runs over time.",
              "Detailed Metadata: For each run, Databricks records the execution context ‚Äî start/end time, duration, compute used, and run parameters.",
              "Error Handling: Provides error codes and logs to diagnose failure causes (e.g., cluster issues, data errors, script exceptions).",
              "Audit & Compliance: Maintains a complete audit trail for all pipeline executions ‚Äî critical for production governance."
            ]
          },
          {
            "type": "heading",
            "text": "What You See:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Start time ‚Üí When the job started.",
              "Job name ‚Üí Which job ran (for example, ‚ÄúETL Pipeline‚Äù).",
              "Run as ‚Üí Which user or role ran it.",
              "Duration ‚Üí How long it took.",
              "Status ‚Üí Shows if it succeeded, failed, or skipped.",
              "Error code ‚Üí Displays the error message if something failed.",
              "Run parameters ‚Üí Lists any input values (like parameters) used in that run."
            ]
          },
          {
            "type": "heading",
            "text": "Compute (Clusters)",
            "heading_level": 2
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-27.png",
            "alt": "Image 27"
          },
          {
            "type": "heading",
            "text": "Compute Categories",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "All-Purpose Compute (Interactive Clusters):",
              "Designed for notebook-driven, collaborative data exploration.",
              "Supports multi-user access, auto-scaling, and auto-termination.",
              "Ideal for data science, ad-hoc analysis, and ML development.",
              "Job Compute (Automated Clusters):",
              "Spawned by the Jobs API or Databricks Workflows for pipeline orchestration.",
              "Clusters are automatically created, executed, and terminated per job run.",
              "Ideal for CI/CD, ETL, and production pipelines.",
              "SQL Warehouses (Serverless and Classic):",
              "Purpose-built compute for data analysts and BI tools.",
              "Integrates with Power BI, Tableau, and Databricks SQL Dashboards.",
              "Serverless option scales automatically and charges only for query duration.",
              "Vector Search & Lakehouse AI (new additions):",
              "Supports AI/ML model deployment, feature lookups, and semantic search.",
              "Works with Unity Catalog and Model Serving endpoints for production AI systems."
            ]
          },
          {
            "type": "heading",
            "text": "Marketplace",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Databricks Marketplace is a data and AI exchange platform that allows users to discover, share, and monetize datasets, AI models, and notebooks within the Databricks Lakehouse ecosystem ‚Äî all powered by Delta Sharing (the open standard for secure data sharing).",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs designed to make it easy for organizations to:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Access third-party datasets (financial, marketing, healthcare, etc.)",
              "Share their own data products securely",
              "Speed up analytics and AI innovation without complex data integrations"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-28.png",
            "alt": "Image 28"
          },
          {
            "type": "heading",
            "text": "Key Components",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Component",
                "Description"
              ],
              [
                "Marketplace Listings",
                "Published datasets, ML models, or notebooks."
              ],
              [
                "Providers",
                "Organizations offering data or AI content (e.g., FactSet, Salesforce)."
              ],
              [
                "Consumers",
                "Databricks users or organizations that subscribe to listings."
              ],
              [
                "Delta Sharing Protocol",
                "Enables secure, open-standard data exchange between different platforms."
              ],
              [
                "Unity Catalog Integration",
                "Ensures governance, lineage, and access control for shared assets."
              ]
            ]
          }
        ]
      }
    ]
  },
  {
    "id": "databricks-sql",
    "title": "Databricks SQL",
    "sections": [
      {
        "title": "SQL Editor",
        "content": [
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-29.png",
            "alt": "Image 29"
          },
          {
            "type": "paragraph",
            "text": "The SQL Editor in Databricks allows users to write, run, and visualize SQL queries directly on data stored in Unity Catalog, Delta tables, or external databases ‚Äî all without needing to create a separate notebook.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs designed for:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data Analysts",
              "BI Developers",
              "Data Engineers",
              "Business users who prefer SQL-based analytics"
            ]
          },
          {
            "type": "paragraph",
            "text": "Think of the SQL Editor as a notepad for data inside Databricks ‚Äîwhere you can write and run SQL commands (like SELECT, JOIN, GROUP BY, etc.) on your company‚Äôs data tables.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs like working in:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "SQL Server Management Studio (SSMS)",
              "or MySQL Workbench ‚Äîbut directly connected to your Databricks Lakehouse."
            ]
          },
          {
            "type": "heading",
            "text": "Key Options",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Run all (1000)",
                "Executes your SQL query. The ‚Äú1000‚Äù indicates the max number of rows returned."
              ],
              [
                "Database Selector (default)",
                "Lets you choose which catalog, schema, or database to query from."
              ],
              [
                "Generate (AI)",
                "Databricks Assistant can auto-generate SQL queries using AI (Ctrl + I)."
              ],
              [
                "Connect",
                "Allows you to choose which SQL warehouse (compute cluster) to run the query on."
              ],
              [
                "Schedule",
                "Lets you set up automated query runs (for reports or alerts)."
              ],
              [
                "Share",
                "Share your query or results with other Databricks users."
              ],
              [
                "Save",
                "Save your query as a draft, dashboard widget, or SQL alert."
              ],
              [
                "Add Parameter",
                "Add variables like dates or IDs dynamically to queries."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Advanced SQL Editor Features",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Description"
              ],
              [
                "AI Assistant (Generate)",
                "Use AI (Ctrl + I) to create SQL automatically from a prompt (e.g., ‚Äúshow top 10 products by revenue‚Äù)."
              ],
              [
                "Visual Output",
                "Query results can be visualized as tables, bar charts, line graphs, etc."
              ],
              [
                "Saved Queries",
                "Queries can be stored and reused from the ‚ÄúQueries‚Äù tab."
              ],
              [
                "Query Parameters",
                "Dynamic filters can be used for dashboards and alerts."
              ],
              [
                "Scheduling & Alerts",
                "Run queries hourly/daily and send alerts when thresholds are reached."
              ],
              [
                "Integration with SQL Warehouses",
                "Choose a compute cluster optimized for BI workloads."
              ],
              [
                "Export Options",
                "Export results as CSV or share within a dashboard."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Professional Use Cases",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Role",
                "Example Use Case"
              ],
              [
                "Data Analyst",
                "Ad-hoc query and visualization for business reports"
              ],
              [
                "Data Engineer",
                "Validate Delta table transformations"
              ],
              [
                "BI Developer",
                "Build dashboards directly from SQL Editor"
              ],
              [
                "Data Scientist",
                "Fetch clean subsets of data for ML notebooks"
              ],
              [
                "Manager / Stakeholder",
                "View high-level KPIs in SQL dashboards"
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Queries",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Queries interface lets you develop and manage SQL statements that interact directly with data in Databricks SQL Warehouses. You can track query execution history, collaborate with team members, tag queries for organization, and use scheduling for automated reporting.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-30.png",
            "alt": "Image 30"
          },
          {
            "type": "heading",
            "text": "Available Options in the Queries Section",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Create Query ‚Äì Opens a new SQL editor window where you can start writing SQL statements.",
              "Open Editor ‚Äì Quickly navigate back to the SQL editor to modify existing queries.",
              "Filter Queries ‚Äì Search for queries by name or tag.",
              "Tabs:",
              "My Queries ‚Äì Shows only your saved queries.",
              "Favorites ‚Äì Displays queries you‚Äôve marked as important.",
              "All Queries ‚Äì Lists all available queries within the workspace.",
              "Created By / Created At ‚Äì Helps you identify who created the query and when.",
              "Query History ‚Äì Access past runs, view execution times, and troubleshoot failed queries.",
              "Dashboards Integration ‚Äì Save query results and directly add them to dashboards for visualization."
            ]
          }
        ]
      },
      {
        "title": "Dashboards",
        "content": [
          {
            "type": "paragraph",
            "text": "Databricks Dashboards provide a powerful visualization layer built directly on top of Databricks SQL. They support real-time data refresh, query scheduling, and access control for collaboration.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "You can embed dashboards in other apps or share them securely within your workspace. It‚Äôs great for operational monitoring, BI reporting, and executive summaries.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-31.png",
            "alt": "Image 31"
          },
          {
            "type": "heading",
            "text": "Options and Features in the Dashboard Section",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Create Dashboard ‚Äì Start building your own dashboard from scratch using your saved queries or visualizations.",
              "View Samples Gallery ‚Äì Explore prebuilt sample dashboards such as NYC Taxi Trip Analysis and Retail Revenue & Supply Chain to understand layout and visualization options.",
              "Filter Dashboards ‚Äì Quickly search for dashboards by name or owner.",
              "Tabs:",
              "All ‚Äì Displays every dashboard you have access to.",
              "Favorites ‚Äì Your bookmarked dashboards.",
              "Popular ‚Äì Dashboards frequently viewed by others.",
              "Last Modified / Owner Filters ‚Äì Sort and manage dashboards based on activity or ownership.",
              "Legacy Dashboards ‚Äì View or migrate older dashboards built using the classic interface.",
              "Visualization Types Supported:",
              "Bar, Line, Area, and Pie charts",
              "Scatter plots and maps",
              "Summary tables and KPI cards",
              "Integration:",
              "Link dashboards directly to Queries or Notebooks",
              "Automate data refresh schedules",
              "Share via workspace or URL"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-32.png",
            "alt": "Image 32"
          },
          {
            "type": "paragraph",
            "text": "Legacy Dashboards in Databricks are maintained mainly for backward compatibility. They support dashboards created with the classic Databricks SQL editor.While functional, they lack newer visualization features, layout flexibility, and integration capabilities present in the modern dashboards.It‚Äôs recommended to migrate older dashboards to the new dashboarding experience for improved performance, interactivity, and long-term support.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Options and Features (Legacy Dashboards Section)",
            "heading_level": 2
          },
          {
            "type": "heading",
            "text": "Tabs and Filters:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "My Dashboards ‚Äì Dashboards you‚Äôve created.",
              "Favorites ‚Äì Frequently used dashboards you‚Äôve bookmarked.",
              "All Dashboards ‚Äì View dashboards shared across your workspace.",
              "Filter Dashboards ‚Äì Quickly locate dashboards by name, creator, or tag."
            ]
          },
          {
            "type": "heading",
            "text": "Actions Available:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "View Samples Gallery ‚Äì Explore sample dashboards with built-in charts and data models.",
              "Create Dashboard ‚Äì Create a new dashboard (though it‚Äôs best to use the new dashboard UI)."
            ]
          },
          {
            "type": "heading",
            "text": "Legacy Dashboard Use Cases:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Maintaining compatibility with older workflows",
              "Referencing historical SQL visualizations",
              "Supporting BI users during migration to new dashboards"
            ]
          },
          {
            "type": "heading",
            "text": "Migration Note:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Databricks encourages using the new ‚ÄúDashboards‚Äù tab for creating interactive and shareable reports.",
              "The new version offers drag-and-drop editing, better visuals, and integrations with Databricks SQL queries and alerts."
            ]
          }
        ]
      },
      {
        "title": "Genie",
        "content": [
          {
            "type": "paragraph",
            "text": "Databricks Genie is a Generative AI-powered assistant built into the Databricks SQL workspace.It allows users to ask questions about data using natural language (like English sentences) ‚Äî and Genie automatically generates SQL queries, runs them, and visualizes the results.",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Genie uses natural language understanding (NLU) to parse questions and generate optimized SQL queries based on data catalog metadata.",
              "It can work across Unity Catalog, SQL Warehouses, and Delta Tables.",
              "Ideal for:",
              "Data analysts exploring ad hoc questions",
              "Business users performing self-service analytics",
              "Teams collaborating in Genie ‚ÄúSpaces‚Äù to share question-answer results"
            ]
          },
          {
            "type": "heading",
            "text": "Key Options in the Genie Interface (from the screenshot)",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Filter spaces ‚Äì Search for an existing ‚ÄúGenie Space.‚Äù",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "A space is like a shared workspace for Genie conversations."
            ]
          },
          {
            "type": "heading",
            "text": "Tabs:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "All ‚Äì View all Genie spaces accessible to you.",
              "Favorites ‚Äì Quickly access frequently used spaces.",
              "Popular ‚Äì See trending Genie spaces used by your team.",
              "Last Modified ‚Äì Sort by recent updates.",
              "Owner ‚Äì Filter by creator or data owner."
            ]
          },
          {
            "type": "paragraph",
            "text": "New ‚Äì Create a new Genie space to start a natural language query session.",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Add datasets or tables.",
              "Ask AI questions about those datasets.",
              "Save and share results or charts."
            ]
          },
          {
            "type": "heading",
            "text": "Genie Spaces",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A Genie Space is a shared area where you can:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Add datasets or views",
              "Ask natural language questions",
              "Save queries and visualizations",
              "Collaborate with team members"
            ]
          },
          {
            "type": "heading",
            "text": "Advantages of Databricks Genie",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Description"
              ],
              [
                "ü§ñ AI-driven",
                "Converts natural language to accurate SQL"
              ],
              [
                "‚ö° Fast Insights",
                "Quick data exploration without manual queries"
              ],
              [
                "üìà Visualization",
                "Auto-generates charts and dashboards"
              ],
              [
                "üîí Secure",
                "Works with Unity Catalog permissions"
              ],
              [
                "üë• Collaborative",
                "Supports multi-user spaces and shared queries"
              ],
              [
                "üß© Integrated",
                "Works with SQL Warehouses and Delta tables"
              ]
            ]
          }
        ]
      },
      {
        "title": "Alerts",
        "content": [
          {
            "type": "paragraph",
            "text": "Alerts in Azure Databricks help you automatically monitor data conditions or metrics in your SQL queries and get notified when something important changes.They make it easy to track trends, catch issues early, and stay updated without checking dashboards manually.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-33.png",
            "alt": "Image 33"
          },
          {
            "type": "paragraph",
            "text": "Alerts can be connected to SQL queries, dashboards, or KPIs across Unity Catalog datasets.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "You can:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Automate anomaly detection for production data.",
              "Trigger alerts for pipeline monitoring, threshold breaches, or data quality checks.",
              "Integrate alerts into workflow tools like Azure Monitor or Slack using webhooks."
            ]
          },
          {
            "type": "paragraph",
            "text": "Advanced configurations let you:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Adjust the schedule frequency.",
              "Add multiple recipients.",
              "Manage alerts programmatically via the Databricks REST API."
            ]
          },
          {
            "type": "heading",
            "text": "Key Elements in the Alerts UI",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Element",
                "Description"
              ],
              [
                "üîç Filter alerts",
                "Search existing alerts by name or keyword."
              ],
              [
                "üìÅ My alerts / All alerts",
                "Switch between alerts you created and those shared by your team."
              ],
              [
                "üßæ List section",
                "Displays alert name, status, last updated time, creator, and creation date."
              ],
              [
                "‚ûï Create alert",
                "Start setting up a new data alert (SQL query-based)."
              ],
              [
                "‚è™ Previous / Next",
                "Navigate between pages if you have multiple alerts."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Benefits of Using Databricks Alerts",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Benefit"
              ],
              [
                "‚ö° Automated Monitoring",
                "Tracks metrics and thresholds continuously"
              ],
              [
                "üì© Notifications",
                "Sends alerts via email or webhooks"
              ],
              [
                "üë• Collaboration",
                "Share alerts across teams or workspaces"
              ],
              [
                "üîí Secure",
                "Follows Unity Catalog access controls"
              ],
              [
                "üß© Integrated",
                "Works with queries, dashboards, and pipelines"
              ]
            ]
          }
        ]
      },
      {
        "title": "Query History",
        "content": [
          {
            "type": "paragraph",
            "text": "The Query History page in Databricks provides a complete log of all SQL queries executed in your workspace.It helps users monitor performance, debug issues, track usage, and ensure compliance ‚Äî all in one place.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-34.png",
            "alt": "Image 34"
          },
          {
            "type": "paragraph",
            "text": "The Query History view is essential for monitoring performance, auditing, and optimizing workloads:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "You can track resource utilization across multiple SQL warehouses.",
              "It‚Äôs useful for troubleshooting slow-running queries.",
              "The Source column identifies where the query originated:",
              "SQL Editor",
              "Dashboard",
              "Alert",
              "API or Notebook",
              "You can also export query metrics via REST API for deeper analytics.",
              "Integration with Unity Catalog ensures secure tracking of all user-level activity across workspaces."
            ]
          },
          {
            "type": "heading",
            "text": "Key Options and Columns",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Element",
                "Description"
              ],
              [
                "üë§ User",
                "Shows who ran the query (e.g., your email ID). Helps identify the query owner."
              ],
              [
                "üìÖ Date Range (Last 7 days)",
                "Filters query history by time period (e.g., last day, week, month, or custom range)."
              ],
              [
                "‚öôÔ∏è Compute",
                "Filters queries based on the SQL Warehouse or cluster used."
              ],
              [
                "‚è±Ô∏è Duration",
                "Lets you filter by how long queries took to run."
              ],
              [
                "üü¢ Status",
                "Shows whether a query succeeded, failed, or was canceled."
              ],
              [
                "üìú Statement / Statement ID",
                "Displays SQL text and a unique identifier for each run. Useful for debugging or tracking jobs."
              ],
              [
                "üîÑ Refresh / Reset filters",
                "Reloads or clears filters to show all results again."
              ],
              [
                "üßÆ Columns in the table",
                "Includes Query, Started at, Duration, Source, Compute, User ‚Äî all helping in tracking query performance."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Common Use Cases",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Use Case",
                "Description"
              ],
              [
                "üßæ Audit Log",
                "Track which users are querying what data for compliance or governance."
              ],
              [
                "üß† Performance Analysis",
                "Identify long-running queries and optimize them."
              ],
              [
                "‚ö° Troubleshooting",
                "Debug query failures using statement IDs."
              ],
              [
                "üßç‚Äç‚ôÇÔ∏è Collaboration",
                "See who ran what and when for shared datasets."
              ],
              [
                "üîî Alert Review",
                "Review the queries triggered by scheduled alerts or dashboards."
              ]
            ]
          }
        ]
      },
      {
        "title": "SQL Data Warehouse",
        "content": [
          {
            "type": "paragraph",
            "text": "A SQL Warehouse (formerly called SQL Endpoint) is the compute resource in Databricks used to run SQL queries, dashboards, and alerts.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It is designed for data analysts, BI developers, and engineers who work with SQL-based data processing ‚Äî similar to how a cluster runs notebooks, but optimized for SQL workloads.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-1-35.png",
            "alt": "Image 35"
          },
          {
            "type": "heading",
            "text": "Key Components (from Screenshot)",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Section",
                "Description"
              ],
              [
                "üîπ Compute Tab",
                "Displays different compute options: All-purpose compute, Job compute, SQL warehouses, etc."
              ],
              [
                "üîπ SQL Warehouses Tab",
                "Dedicated area to view, start, stop, and manage all SQL Warehouses."
              ],
              [
                "üîπ Filter SQL warehouses",
                "Search and filter warehouses by name."
              ],
              [
                "üîπ Only my SQL warehouses",
                "Show only the warehouses created by you."
              ],
              [
                "üîπ Created by / Size / Status / Type",
                "Shows details about the warehouse (who made it, its size, whether it‚Äôs active, and its type)."
              ],
              [
                "üîπ Create SQL warehouse button",
                "Used to create a new warehouse. Disabled if permissions are limited."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Warehouse Properties (Visible Example)",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Property",
                "Description"
              ],
              [
                "üè∑Ô∏è Name",
                "Serverless Starter Warehouse ‚Äî this is a default pre-configured warehouse."
              ],
              [
                "üë§ Created by",
                "The user who created it (e.g., manoj vemula)."
              ],
              [
                "‚öôÔ∏è Size",
                "Defines compute power (Small, Medium, Large, etc.). Determines speed and cost."
              ],
              [
                "üîÅ Active / Max",
                "Shows how many users or queries are currently running on the warehouse."
              ],
              [
                "‚òÅÔ∏è Type",
                "Serverless ‚Äî means Databricks automatically manages compute resources."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Types of SQL Warehouses",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Type",
                "Description",
                "Use Case"
              ],
              [
                "üß† Serverless SQL Warehouse",
                "Fully managed by Databricks. Scales automatically and starts instantly.",
                "Great for quick analysis and dashboards."
              ],
              [
                "‚öôÔ∏è Classic (Pro) SQL Warehouse",
                "Requires manual scaling and management. You control cluster size and scaling.",
                "Used for enterprise workloads needing more control and predictable cost."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "When You Click ‚ÄúCreate SQL Warehouse‚Äù",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "You can define:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Name of warehouse",
              "Cluster size (e.g., Small, Medium, 2X-Large)",
              "Auto-stop timeout to save costs",
              "Max concurrency (how many queries run at once)",
              "Permissions (who can access or run queries)",
              "Channel (stable, preview, etc.)"
            ]
          },
          {
            "type": "heading",
            "text": "üßÆ Technical Features",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Description"
              ],
              [
                "üöÄ Elastic scaling",
                "Automatically adjusts resources to handle varying workloads."
              ],
              [
                "üí∞ Pay-per-use",
                "Charged per DBU (Databricks Unit) based on compute time."
              ],
              [
                "üìä Optimized for BI Tools",
                "Integrates with Power BI, Tableau, and Looker for live queries."
              ],
              [
                "üß† Serverless Architecture",
                "Starts instantly; no need to wait for cluster startup."
              ],
              [
                "üîí Unity Catalog Integration",
                "Enforces data access control and audit policies."
              ]
            ]
          }
        ]
      }
    ]
  },
  {
    "id": "data-engineering",
    "title": "Data Engineering",
    "sections": [
      {
        "title": "Jobs run's",
        "content": [
          {
            "type": "heading",
            "text": "Jobs run‚Äôs",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Jobs & Pipelines interface in Azure Databricks provides a unified orchestration layer for data engineering and machine learning workflows.It supports job scheduling, dependency management, pipeline orchestration, and execution monitoring.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Features",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Ingestion Pipelines: Automate ingestion from external data sources (databases, APIs, or files).",
              "ETL Pipelines: Design scalable, production-grade ETL processes using SQL, PySpark, or Python.",
              "Jobs: Orchestrate notebooks, workflows, pipelines, and queries; configure parameters, cluster settings, and triggers.",
              "Job Runs Dashboard: Monitor run history, logs, and metrics for troubleshooting and optimization.",
              "Access Control: Manage visibility (‚ÄúOwned by me,‚Äù ‚ÄúAccessible by me‚Äù) to enforce workspace-level governance."
            ]
          },
          {
            "type": "heading",
            "text": "Use Case",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Used by data engineers and ML teams to build end-to-end pipelines ‚Äî from data ingestion to transformation, feature generation, and model retraining ‚Äî all under one environment.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-01.png",
            "alt": "Image 1"
          },
          {
            "type": "paragraph",
            "text": "The Job Runs dashboard in Databricks provides an operational view of scheduled or triggered workflows.It allows engineers and ML teams to monitor, debug, and analyze job executions across environments.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Functionalities",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Run Filtering: Filter runs by job, user, time range, run status, or error code.",
              "Run Visualization: Graph at the top visualizes the number of successful, failed, or skipped runs over time.",
              "Detailed Metadata: For each run, Databricks records the execution context ‚Äî start/end time, duration, compute used, and run parameters.",
              "Error Handling: Provides error codes and logs to diagnose failure causes (e.g., cluster issues, data errors, script exceptions).",
              "Audit & Compliance: Maintains a complete audit trail for all pipeline executions ‚Äî critical for production governance."
            ]
          },
          {
            "type": "heading",
            "text": "What You See:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Start time ‚Üí When the job started.",
              "Job name ‚Üí Which job ran (for example, ‚ÄúETL Pipeline‚Äù).",
              "Run as ‚Üí Which user or role ran it.",
              "Duration ‚Üí How long it took.",
              "Status ‚Üí Shows if it succeeded, failed, or skipped.",
              "Error code ‚Üí Displays the error message if something failed.",
              "Run parameters ‚Üí Lists any input values (like parameters) used in that run."
            ]
          }
        ]
      },
      {
        "title": "Data Ingestion",
        "content": [
          {
            "type": "paragraph",
            "text": "Data ingestion means bringing data into Databricks from different sources ‚Äî databases, APIs, files, or cloud storage ‚Äî so that you can analyze or transform it later.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs the first step in any data pipeline or analytics workflow.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-02.png",
            "alt": "Image 2"
          },
          {
            "type": "paragraph",
            "text": "The Data Ingestion tab acts as a centralized data onboarding interface.It supports:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Direct connectors for enterprise systems",
              "File-based uploads into Unity Catalog-managed storage",
              "Automation tools like Fivetran and Partner Connect",
              "Delta Lake and ADLS integrations for scalable storage"
            ]
          },
          {
            "type": "paragraph",
            "text": "It ensures schema consistency, metadata registration, and secure data governance under Unity Catalog.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Header:‚û°Ô∏è Add data",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Purpose:Guides you to connect data sources, upload files, or create tables for analysis.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Main Sections and Options",
            "heading_level": 2
          },
          {
            "type": "heading",
            "text": "Databricks Connectors",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "These are pre-built connectors to quickly connect to popular data platforms:",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Connector",
                "Description",
                "Typical Use Case"
              ],
              [
                "üîπ Salesforce",
                "Connect to CRM data (leads, opportunities, accounts).",
                "Analyze customer and sales performance."
              ],
              [
                "üîπ SAP Business Data Cloud",
                "Access enterprise resource data from SAP.",
                "Supply chain or financial reporting."
              ],
              [
                "üîπ Workday Reports",
                "Retrieve HR, payroll, and workforce data.",
                "Workforce analytics and reporting."
              ],
              [
                "üîπ ServiceNow",
                "Connect IT service management data.",
                "Incident and change management insights."
              ],
              [
                "üîπ Google Analytics Raw Data",
                "Import website and marketing analytics data.",
                "Digital marketing and campaign performance."
              ],
              [
                "üîπ SQL Server",
                "Connect on-prem or cloud-hosted SQL databases.",
                "Bring structured transactional data into Databricks."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Files Section",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "For manual uploads or storage-based ingestion.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description",
                "When to Use"
              ],
              [
                "üì§ Create or modify table",
                "Upload files like CSV, JSON, or Parquet to create or replace tables.",
                "Ideal for one-time imports or small datasets."
              ],
              [
                "üìÅ Upload files to a volume",
                "Upload non-tabular files (images, logs, etc.) managed under Unity Catalog Volumes.",
                "For non-structured data like logs, models, or raw files."
              ],
              [
                "‚òÅÔ∏è Create table from Azure Data Lake Storage (ADLS)",
                "Load data directly from Azure Data Lake into a Delta table.",
                "For large-scale, enterprise-grade data pipelines."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Fivetran Connectors (via Partner Connect)",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "At the bottom, you‚Äôll find:",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚ÄúSee all available ingest partners in Partner Connect.‚Äù",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Partner Connect lets you integrate tools like:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Fivetran, Informatica, Qlik, etc.to automate ingestion from hundreds of data sources into Databricks."
            ]
          }
        ]
      }
    ]
  },
  {
    "id": "ai-ml",
    "title": "AI/ML",
    "sections": [
      {
        "title": "Playground",
        "content": [
          {
            "type": "paragraph",
            "text": "The Playground in Databricks is an interactive environment where you can experiment with AI models, build and test prompts, and prototype intelligent agents before deploying them into production.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs like a sandbox for Generative AI within your Databricks workspace.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-03.png",
            "alt": "Image 3"
          },
          {
            "type": "paragraph",
            "text": "Chat with or test AI models (like GPT, MPT, or Llama).",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Ask questions, summarize documents, or generate code.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Try out small AI tasks (like question answering or summarization) before building real applications.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "A low-code interface for LLM prompt engineering and evaluation.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Integration with Unity Catalog tools for secure, governed model use.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "The ability to prototype AI agents with custom tools, such as function calling, retrieval-augmented generation (RAG), and data-aware AI.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Seamless connection to Databricks‚Äô MLflow, Feature Store, and Model Serving for deployment.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Main Components on the Page",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Section",
                "Description",
                "Purpose"
              ],
              [
                "Model Selector (Top Bar)",
                "Shows the current model (e.g., GPT OSS 120B). You can switch between models here.",
                "Choose which AI model to test or fine-tune."
              ],
              [
                "Tools Menu",
                "Access to tools or APIs integrated with the model (like function calling, RAG, or evaluation tools).",
                "Extend the model‚Äôs capabilities using custom or pre-built tools."
              ],
              [
                "Prototype an Agent",
                "Lets you add your own tool and connect it to a model to create AI agents.",
                "Build task-oriented AI agents (e.g., summarizer, SQL generator, chatbot)."
              ],
              [
                "Start with an Example",
                "Offers quick test templates: Function Calling, Summarization, Document Q&A.",
                "Try example scenarios to understand model behavior."
              ],
              [
                "Evaluation Section",
                "Helps evaluate model responses.",
                "Assess accuracy, relevance, and quality of model outputs."
              ],
              [
                "Prompt Input Area",
                "Text box at the bottom (\"Start typing...\").",
                "Enter prompts, run queries, and see model responses interactively."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "For AI Developers",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Playground supports:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Unity Catalog AI Tools ‚Äî governed access to enterprise data.",
              "Databricks Foundation Models ‚Äî like MPT, Llama 2, GPT OSS, etc.",
              "Custom Tool Integration ‚Äî connect APIs or databases to your AI agent.",
              "Prompt Evaluation ‚Äî test, compare, and optimize prompts before production use."
            ]
          },
          {
            "type": "heading",
            "text": "Advanced Features",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Description"
              ],
              [
                "üß© Agent Prototyping",
                "Create and test agents that can use APIs, databases, or documents."
              ],
              [
                "üîó Function Calling",
                "Extend the model‚Äôs capabilities by allowing it to call your defined Python or SQL functions."
              ],
              [
                "üßæ Prompt Testing",
                "Evaluate how prompts perform across models."
              ],
              [
                "üìä Evaluation Tools",
                "Use built-in metrics to test model quality (accuracy, bias, hallucination rate)."
              ],
              [
                "üîê Unity Catalog Integration",
                "Ensure data governance and secure access during AI experiments."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Benefits",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Benefit",
                "Description"
              ],
              [
                "üß† Hands-on AI Development",
                "Experiment freely without deployment setup."
              ],
              [
                "üîç Prompt Optimization",
                "Refine and evaluate prompts before production."
              ],
              [
                "üß© Custom Tool Integration",
                "Combine AI reasoning with data or APIs."
              ],
              [
                "üîí Governance",
                "Integrated with Unity Catalog for secure and auditable AI testing."
              ],
              [
                "üåê Multiple Model Access",
                "Test open-source and Databricks-hosted LLMs."
              ]
            ]
          }
        ]
      },
      {
        "title": "Experiments",
        "content": [
          {
            "type": "paragraph",
            "text": "In Databricks, Experiments represent the core of model development and tracking.An experiment records each run of your machine learning or AI workflow ‚Äî including:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Model parameters (like learning rate, epochs)",
              "Metrics (like accuracy, loss)",
              "Code version",
              "Data version",
              "Model artifacts (like trained models)"
            ]
          },
          {
            "type": "paragraph",
            "text": "Experiments help track, compare, and reproduce model performance over time using MLflow.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-04.png",
            "alt": "Image 4"
          },
          {
            "type": "paragraph",
            "text": "The Experiments module integrates tightly with MLflow 3, providing:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Unified tracking for ML, DL, and GenAI experiments.",
              "Versioning for both data and models.",
              "Prompt tracking for LLM fine-tuning and evaluation.",
              "Advanced observability ‚Äî including lineage and traceability for GenAI agents."
            ]
          },
          {
            "type": "heading",
            "text": "Visible Sections:",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Section",
                "Description"
              ],
              [
                "üß† GenAI apps & agents",
                "For building and tracking Generative AI apps or AI agents."
              ],
              [
                "üìâ Regression",
                "Create regression models automatically using AutoML."
              ],
              [
                "üîÆ Forecasting (Preview)",
                "Build time-series forecasting models."
              ],
              [
                "üß© Classification",
                "Train classification models (binary or multi-class)."
              ],
              [
                "‚öôÔ∏è Custom model training",
                "For custom classical ML or deep learning experiments."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Experiment Management Options",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Purpose"
              ],
              [
                "üîç Filter experiments",
                "Search for experiments by name, tag, or creator."
              ],
              [
                "üë§ Only my experiments",
                "Shows experiments created by the current user."
              ],
              [
                "üîÅ Reset filters",
                "Clears search and shows all experiments."
              ],
              [
                "üßæ Experiment List Table",
                "Displays Name, Created by, Last modified, Location, and Description."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Core Functionalities",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Function",
                "Description"
              ],
              [
                "üß† AutoML Experiments",
                "Automatically builds and tunes models for regression, classification, or forecasting tasks."
              ],
              [
                "üß© Custom Model Training",
                "Allows full control of model code ‚Äî supports PyTorch, TensorFlow, Scikit-learn, etc."
              ],
              [
                "ü§ñ GenAI & LLM Tracking",
                "Records prompt configurations, LLM outputs, and tool usage for AI agents."
              ],
              [
                "üìä Experiment Comparison",
                "Lets you visually compare multiple runs ‚Äî metrics, parameters, and outputs."
              ],
              [
                "üîó Integration with Feature Store & Models",
                "Once the best model is found, link it to the Model Registry for deployment."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Advanced Capabilities (MLflow 3 + Databricks)",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Capability",
                "Description"
              ],
              [
                "üìò Prompt Versioning",
                "For LLM-based experiments ‚Äî tracks prompt templates and versions."
              ],
              [
                "üß≠ LLM Judges",
                "Evaluate AI outputs automatically (quality, relevance, hallucinations)."
              ],
              [
                "üß© Unified ML + GenAI Tracking",
                "Track classical ML, DL, and GenAI in one interface."
              ],
              [
                "üßæ Agent Tracing",
                "Trace multi-step reasoning of AI agents end-to-end."
              ],
              [
                "üß† Enhanced Model Logging",
                "Log not just metrics, but embeddings, prompts, and responses."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Benefits",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Benefit",
                "Description"
              ],
              [
                "üìä Full visibility",
                "Every model training run is recorded with detailed logs."
              ],
              [
                "üîÅ Reproducibility",
                "Easily re-run any experiment exactly as before."
              ],
              [
                "üîç Comparative analysis",
                "Identify best-performing models visually."
              ],
              [
                "üß© Integration with Models & Serving",
                "Move successful experiments straight to production."
              ],
              [
                "üîê Governance & Version Control",
                "Integrated with Unity Catalog for compliance and traceability."
              ]
            ]
          }
        ]
      },
      {
        "title": "Features",
        "content": [
          {
            "type": "paragraph",
            "text": "In Machine Learning, features are the input variables (columns) used by a model to make predictions.For example:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "In a credit scoring model ‚Üí income, age, loan_amount are features.",
              "In a product recommender ‚Üí user_history, click_rate, category_interest are features."
            ]
          },
          {
            "type": "paragraph",
            "text": "The Features tab in Databricks allows you to manage, share, and reuse these features across models and teams.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-05.png",
            "alt": "Image 5"
          },
          {
            "type": "paragraph",
            "text": "The Feature Store in Databricks (integrated with Unity Catalog) provides:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Centralized feature management",
              "Governed access using Unity Catalog",
              "Feature lineage tracking",
              "Online/offline store integration for model training and real-time inference"
            ]
          },
          {
            "type": "paragraph",
            "text": "It enables feature discovery, reuse, versioning, and monitoring at enterprise scale.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Table Columns (once features exist):",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Column",
                "Description"
              ],
              [
                "Table name",
                "Name of the feature table."
              ],
              [
                "Owner",
                "Who created or owns the feature table."
              ],
              [
                "Online stores",
                "Indicates if the feature is deployed for real-time access."
              ],
              [
                "Last written",
                "Timestamp of the latest update to the feature table."
              ],
              [
                "Tags",
                "Custom tags for searching/grouping features."
              ],
              [
                "Comment",
                "Description or notes about the feature table."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Permissions & Governance",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "You can assign permissions at different levels:",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Level",
                "Example",
                "Purpose"
              ],
              [
                "Catalog",
                "main",
                "Manage data organization"
              ],
              [
                "Schema",
                "retail",
                "Group related feature tables"
              ],
              [
                "Table",
                "customer_features",
                "Control access to specific features"
              ]
            ]
          },
          {
            "type": "paragraph",
            "text": "This governance ensures compliance, security, and collaboration.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Feature Store Components",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Component",
                "Description"
              ],
              [
                "Feature Table",
                "Delta table registered as a feature set."
              ],
              [
                "Feature Lookup",
                "Mapping between input data and feature tables during training or inference."
              ],
              [
                "Training Set",
                "Merged dataset (features + labels) used for model training."
              ],
              [
                "Online Store",
                "Low-latency feature storage for serving models in production."
              ],
              [
                "Feature Lineage",
                "Tracks which data sources and transformations produced each feature."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Key Benefits",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Benefit",
                "Explanation"
              ],
              [
                "‚ôªÔ∏è Feature Reusability",
                "Build once, use everywhere ‚Äî across teams and projects."
              ],
              [
                "üìä Consistency",
                "Same feature logic is applied in training and serving."
              ],
              [
                "üîç Discoverability",
                "Easily search and explore existing features."
              ],
              [
                "üßæ Governance",
                "Controlled via Unity Catalog with fine-grained access control."
              ],
              [
                "üß† Lineage & Audit",
                "Full visibility from raw data ‚Üí feature ‚Üí model ‚Üí prediction."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Advanced Options (for Professionals)",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Capability",
                "Description"
              ],
              [
                "üß© Batch & Streaming Features",
                "Supports both static and streaming feature tables."
              ],
              [
                "üåê Real-time Inference",
                "Integrates with Redis, Cosmos DB, or custom online stores."
              ],
              [
                "üß¨ Feature Monitoring",
                "Detects drift in feature distributions over time."
              ],
              [
                "üß† Feature Versioning",
                "Track and manage changes in feature definitions."
              ],
              [
                "üß∞ Integration with MLflow",
                "Features used in experiments are automatically logged for reproducibility."
              ]
            ]
          }
        ]
      },
      {
        "title": "Models",
        "content": [
          {
            "type": "paragraph",
            "text": "`This section is part of Databricks Machine Learning.  it helps you register, version, manage, and serve ML models built using MLflow or other frameworks.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-06.png",
            "alt": "Image 6"
          },
          {
            "type": "paragraph",
            "text": "Currently, no models are registered yet. But once you create or import models, the table will display:",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Column",
                "Description"
              ],
              [
                "Name",
                "The model‚Äôs registered name."
              ],
              [
                "Catalog",
                "The Unity Catalog that stores the model (e.g., main, sandbox, etc.)."
              ],
              [
                "Schema",
                "The schema inside the catalog that holds the model."
              ],
              [
                "Last Modified",
                "Timestamp of the latest model version update."
              ],
              [
                "Owner",
                "The Databricks user or service principal who owns the model."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Unity Catalog / Workspace Model Registry",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Unity Catalog ‚Üí For governance and access control across workspaces.",
              "Workspace Model Registry ‚Üí Local registry (older style, per workspace)."
            ]
          },
          {
            "type": "paragraph",
            "text": "Owned by Me / Owner Filter ‚Üí Filter to show only models you own or specific users‚Äô models.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Search Bar ‚Üí Search registered models by name.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Once Models Exist ‚Äî More Options Appear",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "When you have registered models, you get these additional actions:",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "1. Model Versioning",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Each model can have multiple versions (v1, v2, ‚Ä¶) for tracking updates or retraining cycles.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "2. Model Staging",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Models can have lifecycle stages:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "None ‚Äì Just registered",
              "Staging ‚Äì For testing and validation",
              "Production ‚Äì For deployment",
              "Archived ‚Äì Old or deprecated versions"
            ]
          },
          {
            "type": "heading",
            "text": "3. Model Lineage & Metadata",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Tracks which experiment/run created the model.",
              "Shows training dataset lineage (via Unity Catalog).",
              "Metadata like tags, parameters, and metrics appear automatically from MLflow."
            ]
          },
          {
            "type": "heading",
            "text": "4. Permissions",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "You can manage who can:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Read or use the model",
              "Transition model stages",
              "Delete or update versions"
            ]
          },
          {
            "type": "heading",
            "text": "5. Serving Integration",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Once a model is registered and approved, you can:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Deploy it to Databricks Model Serving",
              "Expose it via REST API endpoint for predictions",
              "Integrate with Feature Store for consistent feature usage"
            ]
          }
        ]
      },
      {
        "title": "Serving",
        "content": [
          {
            "type": "paragraph",
            "text": "Model Serving in Databricks allows you to:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Deploy ML models (including LLMs) for real-time predictions",
              "Expose them via REST APIs",
              "Serve open-source or external models (like GPT, Llama)",
              "Automatically scale endpoints based on demand",
              "Secure access with Unity Catalog and IAM policies"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-07.png",
            "alt": "Image 7"
          },
          {
            "type": "heading",
            "text": "Details from the Screenshot",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Column",
                "Description"
              ],
              [
                "Name",
                "The name of the deployed serving endpoint."
              ],
              [
                "State",
                "The current deployment status (e.g., Ready, Deploying, Failed)."
              ],
              [
                "Served entities",
                "The specific model or model version being served (e.g., GPT OSS 120B, Llama 4 Maverick)."
              ],
              [
                "Tags",
                "Metadata tags (e.g., Chat, Embeddings)."
              ],
              [
                "Task",
                "The model type or function ‚Äî Chat (LLMs), Embeddings (vectorization), etc."
              ],
              [
                "Created by",
                "User or system who deployed the endpoint."
              ],
              [
                "Last modified",
                "Timestamp of the latest deployment change."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Top Models You See Here",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "These are Databricks-hosted and external foundation models for chat and embeddings tasks:",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Model",
                "Type",
                "Description"
              ],
              [
                "GPT OSS 120B / 20B",
                "Databricks-hosted",
                "Large open-source GPT-style models for conversational AI."
              ],
              [
                "OpenAI GPT-5",
                "External",
                "Connects to OpenAI endpoint for inference (external integration)."
              ],
              [
                "Llama 4 Maverick / Meta Llama Series",
                "Databricks-hosted",
                "Meta Llama models for chat/instruction tasks."
              ],
              [
                "Gemma 3 12B",
                "Databricks-hosted",
                "Google‚Äôs lightweight open LLMs."
              ],
              [
                "BGE / GTE Large (En)",
                "Embedding models",
                "Used for text embedding, retrieval, and similarity tasks."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Actions Available per Model",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Each model endpoint (like GPT, Llama, etc.) gives you options such as:",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Action",
                "Description"
              ],
              [
                "Use",
                "Opens an interface to test the model directly within Databricks."
              ],
              [
                "Copy",
                "Copies the REST API URL and authentication token."
              ],
              [
                "Configure",
                "Modify endpoint settings ‚Äî scale, model version, environment variables."
              ],
              [
                "Create Serving Endpoint",
                "Deploy your own trained model or clone an existing one."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Part of Full AI/ML Lifecycle",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Stage",
                "Databricks Feature"
              ],
              [
                "Data Preparation",
                "Delta Live Tables / Notebooks / Unity Catalog"
              ],
              [
                "Feature Engineering",
                "Features (Feature Store)"
              ],
              [
                "Experiment Tracking",
                "Experiments (MLflow)"
              ],
              [
                "Model Management",
                "Models (Model Registry)"
              ],
              [
                "Model Deployment",
                "Serving (Real-time endpoints)"
              ],
              [
                "Testing & Interaction",
                "Playground (Chat/LLM interaction UI)"
              ]
            ]
          }
        ]
      }
    ]
  },
  {
    "id": "notebook-features",
    "title": "Notebook-level features",
    "sections": [
      {
        "title": "File-level Features",
        "content": [
          {
            "type": "paragraph",
            "text": "File menu contains all options related to creating, managing, importing, exporting, and sharing notebooks in Databricks.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-09.png",
            "alt": "Image 9"
          },
          {
            "type": "heading",
            "text": "üìÇ Databricks Notebook ‚Äì File Menu",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Menu Option",
                "Description & Usage"
              ],
              [
                "üÜï New notebook",
                "Opens a brand-new notebook. You can choose the language (Python, SQL, Scala, or R) and cluster later."
              ],
              [
                "üì• Import‚Ä¶",
                "Allows you to import existing notebooks (from .dbc, .html, .ipynb, or Git repositories)."
              ],
              [
                "üß≠ New notebook dashboard",
                "Creates a dashboard view ‚Äî ideal for visualizations and results presentation, often used in reporting."
              ],
              [
                "üîó Share‚Ä¶",
                "Opens the sharing dialog where you can grant permissions to other users or groups (View, Run, Edit, Manage)."
              ],
              [
                "‚è∞ Schedule‚Ä¶",
                "Lets you schedule notebook runs at set intervals (daily, hourly, etc.) ‚Äî useful for data pipelines or automation."
              ],
              [
                "‚öôÔ∏è Change default cell language‚Ä¶",
                "Sets the default language (Python, SQL, Scala, or R) for new cells in this notebook."
              ],
              [
                "üß† Commit to Git‚Ä¶",
                "Integrates with Git (GitHub, GitLab, Azure Repos) ‚Äî allows version control, branching, and pushing changes."
              ],
              [
                "üåÄ Clone‚Ä¶",
                "Makes an identical copy of the current notebook within the workspace."
              ],
              [
                "‚úèÔ∏è Rename‚Ä¶",
                "Rename the notebook file name."
              ],
              [
                "üì§ Export‚Ä¶",
                "Export notebook in multiple formats:‚Äì HTML (read-only view)‚Äì SOURCE (plain text)‚Äì DBC archive‚Äì IPYNB (Jupyter notebook format)."
              ],
              [
                "üì¶ Move‚Ä¶",
                "Move notebook to a different folder or workspace location."
              ],
              [
                "üóëÔ∏è Move to trash",
                "Deletes (moves) notebook to Trash; can be restored later if needed."
              ],
              [
                "üß± Notebook format ‚Ä∫",
                "Sub-menu to choose export format or convert to different type (e.g., source code, HTML, IPYNB)."
              ],
              [
                "‚¨ÜÔ∏è Upload files to volume‚Ä¶",
                "Uploads files (datasets, scripts, etc.) directly into a mounted volume or workspace for use in your notebook."
              ],
              [
                "üßÆ Create or modify table‚Ä¶",
                "Opens Databricks Data UI to create or edit tables (either Delta or other supported file formats)."
              ],
              [
                "üìä Add data ‚Ä∫",
                "Opens data ingestion options to connect to data sources (Azure Blob, ADLS, Delta tables, CSVs, etc.)."
              ]
            ]
          },
          {
            "type": "paragraph",
            "text": ".",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 1. New Notebook",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Creates a new Databricks notebook.",
              "You can choose the language (Python, SQL, Scala, R).",
              "Optionally select a cluster to attach before running code."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 2. Import",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Used to import existing notebooks or code files.",
              "Supported formats:",
              ".dbc (Databricks archive)",
              ".py, .ipynb, .r, .scala",
              "You can import from:",
              "Local machine",
              "URL",
              "Git repository"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 3. New Notebook Dashboard",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Creates a Dashboard view for notebook outputs.",
              "Dashboards allow you to pin visualizations and results from one or more notebooks to share with others (useful for reporting)."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 4. Share",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Opens the sharing dialog to control notebook access.",
              "You can:",
              "Add collaborators (with View / Edit / Run permissions)",
              "Share via workspace, link, or group",
              "Control access to comments and output visibility"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 5. Schedule",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Allows you to automate notebook runs.",
              "You can:",
              "Set time-based schedules (daily, weekly, etc.)",
              "Configure cluster and parameters for each run",
              "Send email notifications or webhook alerts upon completion/failure"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 6. Change Default Cell Language",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Lets you choose the default programming language for all new cells in the notebook.",
              "Supported languages:",
              "Python",
              "SQL",
              "Scala",
              "R"
            ]
          },
          {
            "type": "paragraph",
            "text": "(You can still mix languages using cell magic commands like %python, %sql, etc.)",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 7. Commit to Git",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Connects the notebook to a Git repository (GitHub, Azure DevOps, GitLab, Bitbucket, etc.).",
              "Allows you to:",
              "Commit changes",
              "View revision history",
              "Revert or pull updates"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 8. Clone",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Creates a copy of the notebook in the same or another workspace folder.",
              "Useful for creating backup or template versions."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 9. Rename",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Renames the current notebook file."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 10. Export",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Exports the current notebook in one of several formats:",
              ".dbc (Databricks archive)",
              ".html (for static sharing)",
              ".ipynb (Jupyter-compatible)",
              ".source (plain code export)"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 11. Move",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Moves the notebook to a different workspace folder or directory."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 12. Move to Trash",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Sends the notebook to the Trash folder in Databricks.",
              "You can later restore or permanently delete it."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 13. Notebook Format",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Selects how notebook content is saved and versioned.",
              "Two options:",
              "Source file format: saves as plain text (easier for Git versioning).",
              "DBC archive format: compressed binary (used for backups)."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 14. Upload Files to Volume",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Uploads local files (like CSVs, JSONs, etc.) directly to:",
              "DBFS (Databricks File System) or",
              "Mounted Azure Storage / AWS S3 / GCP bucket."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 15. Create or Modify Table",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Opens the Data Import Wizard.",
              "You can load data files (CSV, Parquet, JSON, etc.) into a table in:",
              "Delta Lake",
              "Hive Metastore",
              "Unity Catalog"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 16. Add Data",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Shortcut to Databricks Add Data interface, which lets you:",
              "Browse uploaded files",
              "Connect to external data sources (SQL DBs, cloud storage)",
              "Automatically create tables from uploaded data"
            ]
          }
        ]
      },
      {
        "title": "Edit level features",
        "content": [
          {
            "type": "paragraph",
            "text": "Edit menu provides tools for:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Managing and rearranging cells",
              "Editing and formatting code",
              "Controlling execution flow (skip/unskip)",
              "Performing notebook-wide formatting and search",
              "Setting parameters for automated or reusable notebooks"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-10.png",
            "alt": "Image 10"
          },
          {
            "type": "heading",
            "text": "Databricks Notebook ‚Äì Edit Menu",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Menu Option",
                "Shortcut (Windows/Linux)",
                "Description"
              ],
              [
                "Undo",
                "Ctrl + Z",
                "Reverses the most recent cell edit or deletion."
              ],
              [
                "Cut cell(s)",
                "Ctrl + X",
                "Removes the selected cell(s) and copies them to the clipboard."
              ],
              [
                "Copy cell(s)",
                "Ctrl + C",
                "Copies selected cell(s) to the clipboard."
              ],
              [
                "Paste cell(s)",
                "Ctrl + V",
                "Pastes the copied or cut cell(s) below the current one."
              ],
              [
                "Delete cell(s)",
                "D, D",
                "Deletes the currently active cell(s). Press D twice quickly."
              ],
              [
                "Skip/Unskip cell(s)",
                "Ctrl + /",
                "Marks a cell to be skipped during ‚ÄúRun All‚Äù execution, or unskips it. Useful for temporarily disabling code."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Cell Insertion and Arrangement",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Menu Option",
                "Shortcut",
                "Description"
              ],
              [
                "Insert cell above",
                "A",
                "Adds a new blank cell above the current one."
              ],
              [
                "Insert cell below",
                "B",
                "Adds a new blank cell below the current one."
              ],
              [
                "Move cell up",
                "Ctrl + Alt + ‚Üë",
                "Moves the selected cell upward in the notebook."
              ],
              [
                "Move cell down",
                "Ctrl + Alt + ‚Üì",
                "Moves the selected cell downward in the notebook."
              ],
              [
                "Select all cells",
                "Ctrl + A",
                "Selects every cell in the notebook for bulk operations."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Formatting and Structure",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Menu Option",
                "Shortcut",
                "Description"
              ],
              [
                "Format cell(s)",
                "Ctrl + Shift + F",
                "Auto-formats code in the current cell (indentation, spacing)."
              ],
              [
                "Format notebook",
                "‚Äî",
                "Automatically formats all cells for consistent code style."
              ],
              [
                "Notebook Python indentation‚Ä¶",
                "‚Äî",
                "Opens indentation settings for Python cells (tabs vs spaces, size)."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "‚öôÔ∏è Additional Utilities",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Menu Option",
                "Shortcut",
                "Description"
              ],
              [
                "Add parameter‚Ä¶",
                "‚Äî",
                "Allows you to define notebook parameters (for parameterized runs using widgets)."
              ],
              [
                "Find‚Ä¶",
                "Ctrl + F",
                "Opens a search bar to find specific words, code, or text within the notebook."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 1. Undo / Redo",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Undo (Ctrl + Z): Reverts your last action (e.g., deleting a cell, changing code).",
              "Redo (Ctrl + Shift + Z): Re-applies an undone action."
            ]
          },
          {
            "type": "paragraph",
            "text": "üß† Useful for quickly fixing mistakes during editing.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 2. Cut / Copy / Paste Cells",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Cut Cell: Removes the selected cell and stores it in clipboard.",
              "Copy Cell: Copies the selected cell‚Äôs content to clipboard.",
              "Paste Cell Below / Above: Inserts the copied/cut cell at a specific position."
            ]
          },
          {
            "type": "paragraph",
            "text": "üìã Helps in reorganizing notebook structure efficiently.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 3. Move Cell Up / Down",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Moves the selected cell vertically within the notebook.",
              "Maintains execution order when rearranging your logic."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 4. Delete Cell",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Permanently deletes the selected cell from the notebook.",
              "Shortcut: Ctrl + D or Click Trash icon on cell toolbar."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 5. Add Cell Above / Below",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Quickly inserts a new empty cell above or below the selected one.",
              "The new cell defaults to the notebook‚Äôs primary language (e.g., Python)."
            ]
          },
          {
            "type": "paragraph",
            "text": "Shortcut:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "A ‚Üí Add Above",
              "B ‚Üí Add Below"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 6. Merge with Cell Above / Below",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Combines the current cell with an adjacent one (above or below).",
              "Helps when you split code accidentally or want cleaner cell grouping."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 7. Split Cell",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Splits the current cell at the cursor position into two new cells.",
              "Keeps the same language type for both new cells."
            ]
          },
          {
            "type": "paragraph",
            "text": "Shortcut: Ctrl + Shift + ‚Äì",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 8. Find and Replace",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Opens a search bar to find text or code patterns in the notebook.",
              "You can also replace text directly.",
              "Supports:",
              "Match Case",
              "Regex search",
              "Replace All"
            ]
          },
          {
            "type": "paragraph",
            "text": "Shortcut: Ctrl + F / Ctrl + H",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 9. Clear Output",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Removes all generated outputs (plots, tables, logs) from the notebook cells.",
              "Options:",
              "Clear Output for Current Cell",
              "Clear Output for All Cells"
            ]
          },
          {
            "type": "paragraph",
            "text": "üßπ Good practice before sharing or committing notebooks to Git.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 10. Clear State",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Resets the Python/Scala/R session (similar to restarting the kernel).",
              "Clears all variables, imports, and cached data."
            ]
          },
          {
            "type": "paragraph",
            "text": "üîÑ Use when memory is full or to rerun from a clean state.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 11. Run Selected Cell / All Cells",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Executes:",
              "Current Cell",
              "All Cells Above / Below",
              "All Cells in Notebook"
            ]
          },
          {
            "type": "paragraph",
            "text": "Shortcut:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Shift + Enter ‚Üí Run and move to next cell",
              "Ctrl + Enter ‚Üí Run cell only",
              "Alt + Enter ‚Üí Run and insert new cell below"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 12. Convert Cell Type",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Converts between:",
              "Code Cell",
              "Markdown Cell",
              "Useful for adding formatted text, documentation, or section titles."
            ]
          },
          {
            "type": "paragraph",
            "text": "Shortcut:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Ctrl + M M (convert to markdown)",
              "Ctrl + M Y (convert to code)"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 13. Comment / Uncomment",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Toggles comment lines in a code cell.",
              "Automatically detects language syntax (Python #, SQL --, etc.).Shortcut: Ctrl + /"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 14. Format Cell Code",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Auto-formats code indentation and spacing.",
              "Improves readability (especially in long notebooks).Shortcut: Ctrl + Shift + F"
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 15. Toggle Line Numbers",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Shows or hides line numbers in code cells.",
              "Helpful for debugging or referencing code during collaboration."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 16. Toggle Output Visibility",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Hides or shows the output section of a cell (results, plots, etc.).",
              "Useful for compact viewing when outputs are large."
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 17. Toggle Comments Panel",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Shows the collaboration comment sidebar.",
              "Lets you add inline comments, tag teammates, or resolve feedback."
            ]
          }
        ]
      },
      {
        "title": "View level features",
        "content": [
          {
            "type": "paragraph",
            "text": "View menu allows you to:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Customize notebook interface and theme",
              "Control code and output visibility",
              "Manage cluster tools and developer options",
              "Access reusable SQL query snippets"
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-11.png",
            "alt": "Image 11"
          },
          {
            "type": "heading",
            "text": "Databricks Notebook ‚Äì View Menu Overview",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The View menu allows you to customize the notebook interface, control UI layout, and access developer or cluster tools.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Views",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "This submenu lets you switch between various Databricks notebook views.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Command Mode / Edit Mode",
                "Switch between cell editing (Enter) and command mode (Esc)."
              ],
              [
                "Presentation Mode",
                "Displays notebook in a clean, full-screen layout ‚Äî useful for demos or teaching."
              ],
              [
                "Code-only / Results-only view",
                "Toggle between hiding code cells or outputs for a focused view."
              ],
              [
                "Show Line Numbers",
                "Enables/disables line numbers in code cells for easier debugging."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Notebook Layout",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Adjusts the overall layout of your Databricks notebook window.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Default layout",
                "Standard view with toolbar, output, and code cells visible."
              ],
              [
                "Compact layout",
                "Reduces padding and spacing for denser code display."
              ],
              [
                "Wide layout",
                "Expands notebook width ‚Äî useful for long code lines or wide tables."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Cell Layout",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Controls how individual notebook cells appear.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Show cell toolbar",
                "Enables a toolbar on each cell for easy access to actions (move, delete, edit)."
              ],
              [
                "Show output by default",
                "Displays the output section for all cells after execution."
              ],
              [
                "Collapse code/output",
                "Collapses code or output areas ‚Äî useful for large notebooks."
              ],
              [
                "Show execution time",
                "Displays how long each cell took to execute."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Workspace Theme",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Controls the Databricks workspace UI color theme.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Light Theme",
                "Bright background; default mode."
              ],
              [
                "Dark Theme",
                "Dark background; reduces eye strain during long sessions."
              ],
              [
                "System Default",
                "Adapts to your OS theme setting automatically."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Editor Theme",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Changes the theme used inside the code editor (cell area).",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Monokai / Solarized / Light / Dark",
                "Various syntax-highlighting color schemes for the code editor."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Side Panel",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Controls the visibility and behavior of the left sidebar (Workspace, Recents, Catalog, etc.).",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Show / Hide side panel",
                "Toggles the entire left sidebar."
              ],
              [
                "Pin / Unpin panel",
                "Keeps the sidebar visible or allows it to auto-hide."
              ],
              [
                "Resize panel",
                "Adjusts the sidebar width."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Appearance",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Manages minor UI preferences.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Show toolbars",
                "Toggles the notebook‚Äôs top toolbar (Run, Connect, Share buttons)."
              ],
              [
                "Show command palette shortcut tips",
                "Enables small shortcut hints under the notebook."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Cluster Tools",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Provides quick access to Databricks cluster monitoring and management.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "View Cluster Logs",
                "Opens logs for the currently attached compute cluster."
              ],
              [
                "Cluster Details",
                "Opens the attached cluster‚Äôs configuration (runtime, libraries, etc.)."
              ],
              [
                "Driver & Worker info",
                "Displays performance metrics for driver and worker nodes."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Developer Settings",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Used for advanced users or developers to customize notebook behavior.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Enable Developer Mode",
                "Turns on advanced tools such as HTML inspector, custom scripts, etc."
              ],
              [
                "Show debug console",
                "Displays developer console for debugging front-end issues."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Query Snippets",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Opens Databricks‚Äô reusable SQL code snippets library.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Option",
                "Description"
              ],
              [
                "Query Snippets window",
                "Opens a new panel with predefined or custom SQL templates for queries."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "üîπ 1. Views",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Provides options to change the overall view of your notebook interface.",
              "Common options:",
              "Command mode view: shows toolbar and menus.",
              "Presentation view: hides toolbars and shows only notebook content for presentations.",
              "Full-screen view: expands the notebook to use full display space."
            ]
          },
          {
            "type": "paragraph",
            "text": "üß† Best for teaching sessions, demos, or focusing on code.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 2. Notebook Layout",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Controls how your notebook‚Äôs interface is displayed.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options include:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Show/Hide Cell Toolbar ‚Äì toggle visibility of the toolbar above each cell.",
              "Show Line Numbers ‚Äì display line numbers inside code cells.",
              "Collapse/Expand Output ‚Äì hide or show outputs (plots, tables, etc.).",
              "Enable Output Scrolling ‚Äì adds scrollbars for long outputs instead of overflowing."
            ]
          },
          {
            "type": "paragraph",
            "text": "üí° Helps declutter large notebooks or improve visual clarity.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 3. Cell Layout",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Adjusts the way individual cells are displayed and interacted with.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options include:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Toggle Input Visibility ‚Äì hide or show the input (code) area of a cell.",
              "Toggle Output Visibility ‚Äì hide or show only the results/output area.",
              "Wrap Text in Cells ‚Äì wraps long lines instead of horizontal scrolling."
            ]
          },
          {
            "type": "paragraph",
            "text": "üìã Ideal when reviewing code or hiding logic for visual focus.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 4. Workspace Theme",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Change the overall Databricks interface color scheme.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Light Theme",
              "Dark Theme"
            ]
          },
          {
            "type": "paragraph",
            "text": "üåó Switch depending on comfort or lighting conditions.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 5. Editor Theme",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Changes the syntax highlighting style inside the code editor only.(Independent of workspace theme.)",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options often include:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Default",
              "Monokai",
              "Solarized Light/Dark",
              "High Contrast"
            ]
          },
          {
            "type": "paragraph",
            "text": "üé® Useful for developers who prefer specific color styles.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 6. Side Panel",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Manage visibility of left-hand and right-hand navigation panels.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Show/Hide Sidebar ‚Äì toggle workspace navigation bar.",
              "Show File Browser / Data / Clusters / Jobs panels.",
              "Show Comments / Insights Panel on right side."
            ]
          },
          {
            "type": "paragraph",
            "text": "üß± Helpful for focusing or expanding workspace tools.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 7. Appearance",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Fine-tunes display settings related to text size and spacing.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Zoom In / Zoom Out ‚Äì increase or decrease text size.",
              "Reset Zoom ‚Äì return to default.",
              "Toggle Compact Mode ‚Äì reduce spacing between cells."
            ]
          },
          {
            "type": "paragraph",
            "text": "üëì Improves readability or space management.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 8. Cluster Tools",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Allows quick visibility of cluster-related panels.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Show Cluster Info Panel ‚Äì view current attached cluster details.",
              "Show Logs / Metrics ‚Äì open cluster monitoring or Spark UI links."
            ]
          },
          {
            "type": "paragraph",
            "text": "‚öôÔ∏è Useful for debugging job or runtime issues directly from notebook.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 9. Developer Settings",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Opens advanced options for developers such as:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Enable Experimental Features",
              "Enable Command Palette Shortcuts",
              "Enable AI Assistant Beta (if available)"
            ]
          },
          {
            "type": "paragraph",
            "text": "üß™ For developers testing new Databricks capabilities.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "üîπ 10. Query Snippets",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Opens pre-saved reusable SQL or Python code snippets.",
              "You can view, insert, or manage snippets for faster notebook development."
            ]
          },
          {
            "type": "paragraph",
            "text": "‚ö° Saves time by reusing common queries or functions.",
            "heading_level": null
          }
        ]
      },
      {
        "title": "Run-level features",
        "content": [
          {
            "type": "paragraph",
            "text": "The Run menu in Databricks allows you to execute, debug, clear outputs, and control compute sessions in your notebook.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs primarily used for running code cells and managing the execution environment.",
            "heading_level": null
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-12.png",
            "alt": "Image 12"
          },
          {
            "type": "heading",
            "text": "Run and Debug",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "This section provides options to execute cells in different ways.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Run Cell / Run All Cells ‚Üí Executes the selected or all notebook cells sequentially.",
              "Run Cell and Move to Next ‚Üí Runs the current cell, then automatically jumps to the next one.",
              "Run Above / Run Below ‚Üí Executes all cells either above or below the currently selected cell.",
              "Debug Cell (if enabled) ‚Üí Allows step-by-step execution for debugging purposes."
            ]
          },
          {
            "type": "paragraph",
            "text": "üí° Use when testing logic or running the whole notebook for analysis or data processing.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Clear",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "This section helps you remove outputs or states from the notebook.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Options:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Clear Output of Current Cell ‚Üí Removes the result/output displayed for the current cell.",
              "Clear Output of All Cells ‚Üí Clears all results throughout the notebook (code remains intact).",
              "Clear State ‚Üí Resets notebook variables or execution state (optional in some setups)."
            ]
          },
          {
            "type": "paragraph",
            "text": "üßπ Useful before re-running code to avoid confusion from old outputs.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Go to Last Run Cell",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Jumps directly to the last executed cell in the notebook.",
              "Helps quickly find where you last left off in a long notebook."
            ]
          },
          {
            "type": "paragraph",
            "text": "üîÅ Very handy for debugging or when resuming work after a pause.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Interrupt Execution (Shortcut: I, I)",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Immediately stops a running cell or notebook execution.",
              "Similar to ‚ÄúStop‚Äù or ‚ÄúCancel‚Äù in other IDEs.",
              "Use this if a cell is taking too long or stuck in an infinite loop."
            ]
          },
          {
            "type": "paragraph",
            "text": "‚èπÔ∏è Prevents resource wastage or cluster overload.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Detach from Compute Resource",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Disconnects the notebook from the currently attached Databricks cluster or compute resource.",
              "After detaching, no code execution can occur until reconnected."
            ]
          },
          {
            "type": "paragraph",
            "text": "‚öôÔ∏è Used when switching clusters, stopping resources, or cleaning up sessions.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "New Session in Compute Resource",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Starts a fresh session within the currently attached cluster.",
              "Clears the existing Python/Scala/R environment (variables, imports, etc.).",
              "Essentially ‚Äúrestarts the kernel‚Äù for a clean state."
            ]
          },
          {
            "type": "paragraph",
            "text": "üîÑ Useful when environment corruption or dependency conflicts occur.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Menu Option",
                "Purpose",
                "Key Use Case"
              ],
              [
                "Run and Debug",
                "Execute selected/all cells",
                "Run or debug code interactively"
              ],
              [
                "Clear",
                "Remove outputs or execution states",
                "Clean notebook before rerun"
              ],
              [
                "Go to Last Run Cell",
                "Jump to last executed cell",
                "Resume work or debug flow"
              ],
              [
                "Interrupt Execution (I, I)",
                "Stop current execution",
                "Abort long or stuck runs"
              ],
              [
                "Detach from Compute Resource",
                "Disconnect cluster",
                "Switch or stop compute"
              ],
              [
                "New Session in Compute Resource",
                "Restart environment",
                "Start fresh session for clean execution"
              ]
            ]
          }
        ]
      },
      {
        "title": "Help-level features",
        "content": [
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-13.png",
            "alt": "Image 13"
          },
          {
            "type": "heading",
            "text": "Search actions",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Opens a search bar to quickly find and execute commands or actions within the notebook.",
              "Shortcut: Ctrl + Shift + P",
              "Use: This allows users to rapidly search through available actions or commands without navigating through menus."
            ]
          },
          {
            "type": "heading",
            "text": "Keyboard shortcuts",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Displays a list of all available keyboard shortcuts for the notebook.",
              "Shortcut: H (when \"Help\" is active)",
              "Use: This is a helpful guide for users to quickly learn and use shortcuts, speeding up the workflow. Shortcuts might include things like running cells or navigating between them."
            ]
          },
          {
            "type": "heading",
            "text": "Provide feedback",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Opens a prompt where users can provide feedback about their experience with Databricks.",
              "Use: This allows users to share their thoughts or report issues they have encountered while using Databricks."
            ]
          },
          {
            "type": "heading",
            "text": "Ask the Databricks community",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Opens a link to the Databricks community forum or help center where users can ask questions or browse discussions.",
              "Use: This connects users to the community for support, troubleshooting, or knowledge sharing."
            ]
          },
          {
            "type": "heading",
            "text": "Databricks support",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Provides a link to official Databricks support resources, including contact information or technical assistance options.",
              "Use: This feature is for users who need direct, official support for their Databricks environment or facing issues that community help may not address."
            ]
          }
        ]
      },
      {
        "title": "Language-level features",
        "content": [
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-14.png",
            "alt": "Image 14"
          },
          {
            "type": "paragraph",
            "text": "Purpose:Sets the default programming language for the current notebook.All new cells you create will use the selected language automatically, though you can still override it in individual cells.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Available options:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Python",
              "Default and most commonly used option.",
              "Supports libraries like PySpark, pandas, NumPy, matplotlib, etc.",
              "Used for data processing, machine learning, and automation tasks.",
              "SQL",
              "Allows you to write SQL queries directly within the notebook.",
              "Often used for querying data from Delta tables or databases.",
              "Integrates well with Databricks‚Äô data management and visualization tools.",
              "Scala",
              "Used for working directly with Apache Spark‚Äôs core language.",
              "Offers performance advantages and full access to Spark APIs.",
              "Often preferred by data engineers for large-scale data transformations.",
              "R",
              "For data analysis and statistical modeling.",
              "Ideal for data scientists working in R environments.",
              "Supports packages like ggplot2 and dplyr."
            ]
          },
          {
            "type": "paragraph",
            "text": "üí° Usage Tip:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "You can mix languages in a single notebook by prefixing a cell with:",
              "%python",
              "%sql",
              "%scala",
              "%r"
            ]
          }
        ]
      },
      {
        "title": "Others features",
        "content": [
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-15.png",
            "alt": "Image 15"
          },
          {
            "type": "heading",
            "text": "Grid/Outline View Icon",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Opens the notebook‚Äôs table of contents or cell outline view.",
              "Use: Lets you navigate quickly between notebook cells or sections especially helpful in long notebooks."
            ]
          },
          {
            "type": "heading",
            "text": "Run all",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Executes all code cells in the notebook sequentially from top to bottom.",
              "Use: Used when you want to rerun the entire notebook (e.g., after making changes to inputs or variables)."
            ]
          },
          {
            "type": "heading",
            "text": "Connect",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Manages your cluster connection.",
              "Use:",
              "Shows which cluster the notebook is currently attached to.",
              "Lets you connect, disconnect, or switch clusters.",
              "The blue dot next to it means it‚Äôs currently connected."
            ]
          },
          {
            "type": "heading",
            "text": "Schedule",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Used to automate notebook runs.",
              "Use:",
              "You can set up recurring runs (daily, weekly, etc.).",
              "Often used for production tasks like data refreshes or batch jobs."
            ]
          },
          {
            "type": "heading",
            "text": "Share",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Purpose: Manages collaboration and access permissions.",
              "Use:",
              "Lets you share the notebook with teammates.",
              "You can give view, edit, or run permissions."
            ]
          },
          {
            "type": "image",
            "src": "/tutorials/azure/images/databricks-2-16.png",
            "alt": "Image 16"
          },
          {
            "type": "heading",
            "text": "1. Comments",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Feature: Comments allow users to add notes, feedback, or annotations to cells in the notebook. It's an important feature for collaboration.",
              "Functionality: You can click on the comment icon, add comments, and reply to other users' comments. This helps teams work together on the notebook without modifying the actual code."
            ]
          },
          {
            "type": "heading",
            "text": "2. MLflow",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Feature: MLflow is an open-source platform used for managing the machine learning lifecycle. In Databricks, MLflow allows you to track experiments, organize models, and manage versioning and deployment.",
              "Functionality: You can use it to log model parameters, metrics, artifacts, and even manage model registry, which can be useful for tracking different versions of models and their performance."
            ]
          },
          {
            "type": "heading",
            "text": "3. Version History",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Feature: This allows you to view previous versions of the notebook, track changes, and revert to earlier versions if needed.",
              "Functionality: You can see the notebook‚Äôs change history, who made the changes, and when. You can roll back to a specific version if necessary, which is great for tracking progress and restoring work in case of mistakes."
            ]
          },
          {
            "type": "heading",
            "text": "4. Variables",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Feature: In Databricks, variables show the current session's defined variables, including their values. This helps you track what‚Äôs available in the current environment (e.g., the output of cells, the current state of dataframes, etc.).",
              "Functionality: It helps to inspect and manage variables directly from the sidebar. You can view the current state of variables, which is useful when working with large datasets or complex computations."
            ]
          },
          {
            "type": "heading",
            "text": "5. Environment",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Feature: The environment shows the runtime or environment context in which the notebook is executing. This can include the libraries installed, the Python/Scala/SQL environment, and the cluster configuration.",
              "Functionality: It allows you to see which cluster or environment the notebook is using, including the versions of libraries that are installed. If you need a different environment (e.g., upgrading libraries or switching clusters), you can manage this from here."
            ]
          },
          {
            "type": "heading",
            "text": "6. Assistant",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Feature: The assistant likely refers to an AI or virtual assistant integrated into Databricks to help with code suggestions, troubleshooting, and providing guidance within the notebook.",
              "Functionality: You can ask the assistant questions related to the notebook, such as asking for code completion, debugging help, or natural language translation of data queries. It's a helpful tool for improving productivity, especially when working with complex data science tasks."
            ]
          }
        ]
      }
    ]
  },
  {
    "id": "data-lakehouse",
    "title": "Data Lakehouse",
    "sections": [
      {
        "title": "Data Lakehouse Overview",
        "content": [
          {
            "type": "heading",
            "text": "What is a Data Lakehouse?",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A data lakehouse is a modern way to store and manage data. It brings together the best parts of data lakes and data warehouses into one system.",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "From data lakes, it takes flexibility, large-scale storage, and low cost.",
              "From data warehouses, it adds strong data management, reliability, and support for ACID transactions (which make sure data stays accurate and consistent)."
            ]
          },
          {
            "type": "paragraph",
            "text": "In simple terms, a data lakehouse lets you store all kinds of data structured or unstructured‚Äîin one place, and then use that same data for business intelligence (BI) reports, analytics, or machine learning (ML) without moving it around.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "On Azure Databricks, you can build and manage a lakehouse to easily collect, clean, process, and analyze your data all within one platform.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Why the Need for a Lakehouse?",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Traditionally, organizations used:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data Lakes ‚Üí for storing raw, unprocessed data cheaply.",
              "Data Warehouses ‚Üí for structured, cleaned data used in analytics and BI."
            ]
          },
          {
            "type": "paragraph",
            "text": "You had to copy and transform data from one system to the other, which caused:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data duplication",
              "High maintenance cost",
              "Delays in getting insights",
              "Data inconsistency"
            ]
          },
          {
            "type": "paragraph",
            "text": "The lakehouse solves this by combining both into one unified system.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Core Features of a Data Lakehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Here are the main features that make a lakehouse powerful:",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Description"
              ],
              [
                "Unified Storage",
                "All data (raw, semi-structured, structured) is stored in one place."
              ],
              [
                "ACID Transactions",
                "Ensures data accuracy and reliability even during concurrent operations."
              ],
              [
                "Schema Enforcement",
                "Automatically validates and maintains data consistency."
              ],
              [
                "Time Travel / Versioning",
                "You can access previous versions of data for audit or rollback."
              ],
              [
                "Data Governance & Security",
                "Centralized access control, auditing, and data lineage."
              ],
              [
                "Open Format (like Delta Lake)",
                "Built on open-source formats like Parquet and Delta for compatibility."
              ],
              [
                "Performance Optimization",
                "Uses caching, indexing, and query optimization for faster analytics."
              ],
              [
                "Support for BI & ML",
                "Enables analysts and data scientists to work directly on the same data."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Lakehouse Architecture Overview",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A typical data lakehouse architecture has three main layers:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Storage Layer (Data Lake)",
              "Stores all raw data at scale (in formats like Parquet or Delta).",
              "Example: Azure Data Lake Storage (ADLS).",
              "Management & Governance Layer",
              "Adds schema, metadata management, and ACID transaction control.",
              "Example: Delta Lake or Unity Catalog in Azure Databricks.",
              "Consumption Layer (Analytics & ML)",
              "Data is consumed for reporting, dashboards, machine learning, and AI.",
              "Example: Power BI, MLflow, Databricks notebooks, or Azure Synapse."
            ]
          },
          {
            "type": "heading",
            "text": "Benefits of a Data Lakehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A lakehouse provides several advantages over traditional data systems:",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚úÖ Single Source of Truth ‚Äì All teams work on the same consistent data.‚úÖ Cost Efficiency ‚Äì Uses low-cost object storage instead of expensive warehouse storage.‚úÖ Flexibility ‚Äì Supports all data types and use cases (BI + AI + ML).‚úÖ Scalability ‚Äì Handles huge volumes of data seamlessly.‚úÖ Data Reliability ‚Äì ACID transactions prevent corruption or data loss.‚úÖ Faster Insights ‚Äì Unified architecture means less movement and faster analysis.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Lakehouse on Azure Databricks",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Azure Databricks is one of the best platforms to build and manage a lakehouse. It combines:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Apache Spark for distributed data processing,",
              "Delta Lake for ACID-compliant storage, and",
              "Unity Catalog for centralized governance and security."
            ]
          },
          {
            "type": "heading",
            "text": "Key components:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Data Ingestion: Load data from multiple sources (Azure Blob, SQL, APIs).",
              "Data Processing: Clean, transform, and enrich data using PySpark or SQL.",
              "Data Storage: Store data in Delta Lake format for reliability and performance.",
              "Data Governance: Manage permissions, lineage, and audit using Unity Catalog.",
              "Data Consumption: Use Power BI, MLflow, or Databricks notebooks for analytics and AI."
            ]
          },
          {
            "type": "heading",
            "text": "Example Use Cases",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A data lakehouse can be used for:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Business dashboards and analytics",
              "Real-time data streaming and reporting",
              "Predictive analytics and machine learning",
              "Customer behavior analysis",
              "IoT and sensor data processing"
            ]
          },
          {
            "type": "heading",
            "text": "Lakehouse vs Data Lake vs Data Warehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Over the years, data management systems have evolved from data warehouses, to data lakes, and now to data lakehouses ‚Äî each solving different challenges and enabling new ways to use data for analytics, business intelligence (BI), and machine learning (ML).",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Let‚Äôs understand how these three systems differ and how they complement each other.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Warehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A data warehouse is a structured storage system built mainly for business intelligence (BI) and reporting.It organizes data into predefined tables and schemas so that it‚Äôs clean, consistent, and easy to query using tools like SQL.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key ideas:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Designed for stable and historical data that doesn‚Äôt change often.",
              "Ideal for running BI dashboards and reports that summarize key business metrics.",
              "Queries are optimized for accuracy and reliability but can take time when dealing with large datasets.",
              "Often uses proprietary formats managed by vendors, which can limit flexibility for machine learning or advanced analytics.",
              "On Azure Databricks, data warehousing is enhanced through Databricks SQL, combining warehouse performance with the scalability of the lakehouse."
            ]
          },
          {
            "type": "paragraph",
            "text": "Best for:üìä Structured data, business reporting, and decision-making dashboards.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Lake",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A data lake is a large, low-cost repository that stores all kinds of data ‚Äî structured, semi-structured, or unstructured in its raw form.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Key ideas:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Can hold massive amounts of data from various sources: applications, sensors, mobile apps, social media, and more.",
              "Uses a schema-on-read approach ‚Äî meaning data structure is applied only when it‚Äôs used.",
              "Highly scalable and affordable, perfect for big data processing, data exploration, and machine learning.",
              "However, since data isn‚Äôt cleaned or validated upfront, it may not be ideal for business reporting that needs trusted, structured data."
            ]
          },
          {
            "type": "paragraph",
            "text": "Best for:ü§ñ Data science, machine learning, and storing large volumes of raw data.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Lakehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A data lakehouse is a modern architecture that combines the strengths of both data lakes and data warehouses into one unified system.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "It allows you to work with structured and unstructured data together, supporting both business intelligence and machine learning without needing separate systems.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Key ideas:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Built on open and standard file formats (like Parquet or Delta Lake) for flexibility and compatibility.",
              "Uses advanced indexing, caching, and metadata management for faster queries and consistent performance.",
              "Supports ACID transactions (ensuring data reliability and accuracy).",
              "Enables low-latency queries for BI, while still providing the scale and flexibility required for data science.",
              "On Azure Databricks, the lakehouse architecture makes it possible to analyze all your data from a single platform using Databricks SQL and Delta Lake."
            ]
          },
          {
            "type": "paragraph",
            "text": "Best for:üß© Unified analytics ‚Äî combining BI, AI, and ML on one consistent, governed data foundation.",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Feature",
                "Data Lake",
                "Data Lakehouse",
                "Data Warehouse"
              ],
              [
                "Types of Data",
                "Can store any kind of data ‚Äî structured, semi-structured, or unstructured (raw) data.",
                "Handles all types of data ‚Äî structured, semi-structured, and unstructured ‚Äî in one place.",
                "Mainly stores structured and processed data only."
              ],
              [
                "Cost",
                "üí∞ Very cost-effective for large storage needs.",
                "üí∞ Cost-efficient and scalable for all workloads.",
                "üí∏üí∏ Expensive to scale and maintain due to licensing and vendor costs."
              ],
              [
                "Data Format",
                "Uses open and flexible file formats (like Parquet, JSON, CSV).",
                "Uses open and modern formats (like Delta or Parquet) for compatibility.",
                "Usually based on closed or proprietary formats tied to the vendor."
              ],
              [
                "Scalability",
                "Easily scales to store massive amounts of data at a low cost.",
                "Scales efficiently while maintaining performance and governance.",
                "Scaling is possible but becomes costly as data volume grows."
              ],
              [
                "Main Users",
                "Mostly used by data engineers and data scientists who can handle raw data.",
                "Designed for everyone ‚Äî data analysts, data scientists, and machine learning teams.",
                "Primarily used by business analysts for structured reporting."
              ],
              [
                "Data Quality & Reliability",
                "Can become messy or inconsistent without strong management (sometimes called a ‚Äúdata swamp‚Äù).",
                "Delivers reliable, high-quality, well-managed data.",
                "Provides high-quality and consistent data with strict schema control."
              ],
              [
                "Ease of Use",
                "Harder to use directly ‚Äî raw data requires preparation and organization.",
                "Easy to use ‚Äî combines the simplicity of a warehouse with the flexibility of a lake.",
                "Easy to query and report on, but limited to structured data only."
              ],
              [
                "Performance",
                "Slower for analytics since data is raw and unoptimized.",
                "High performance ‚Äî optimized for both analytics and AI workloads.",
                "High performance for traditional analytics and BI."
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Capabilities of a Databricks Lakehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A Databricks Lakehouse brings together the best features of data lakes and data warehouses into a single, powerful platform.It removes the need to maintain separate systems for analytics, machine learning (ML), and business intelligence (BI), helping organizations manage all their data workloads in one unified environment.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Here are the major capabilities that make the Databricks Lakehouse stand out:",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Real-Time Data Processing",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Process and analyze streaming data in real time whether it‚Äôs coming from IoT devices, logs, or event streams.This allows instant insights and faster decision-making instead of waiting for batch jobs to complete.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Unified Data Integration",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Bring all your organization‚Äôs data ‚Äî structured, semi-structured, and unstructured ‚Äî into a single, centralized platform.This creates a single source of truth, improves collaboration between teams, and reduces data silos.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Schema Evolution",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Easily modify and update data schemas as your business changes, without breaking existing pipelines.Databricks automatically adapts to evolving data structures, ensuring flexibility and smooth operations over time.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Fast and Reliable Data Transformations",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "With Apache Spark and Delta Lake, you can perform large-scale data transformations quickly and reliably.This makes data preparation, cleaning, and enrichment faster and more efficient for analytics and ML workflows.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Advanced Data Analysis and Reporting",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Run complex analytical queries with performance comparable to a traditional data warehouse.Databricks‚Äô query engine is optimized for data warehousing workloads, enabling fast dashboards and deep analysis.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Machine Learning and AI",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Apply machine learning and artificial intelligence directly on your data within the same platform.You can train, test, and deploy ML models using Databricks‚Äô built-in integrations with MLflow, Delta Lake, and other AI frameworks.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Versioning and Lineage",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Databricks supports data version control, allowing you to access or roll back to previous versions of a dataset.You can also track data lineage, ensuring full transparency about where data comes from and how it has changed over time.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Governance and Security",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Manage permissions, access control, and auditing from one place using Unity Catalog.This ensures compliance, security, and proper governance across all teams and workloads.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Sharing and Collaboration",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Share curated datasets, dashboards, and insights securely across departments or with external partners.The lakehouse supports controlled, real-time data sharing ‚Äî no need for data duplication or exports.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Operational Analytics and Monitoring",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Continuously monitor data quality, model accuracy, and performance drift using built-in data quality tools.This helps maintain reliability and trust in your analytics and machine learning outputs.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "scope of the Lakehouse platform",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Lakehouse Platform represents the next generation of data architecture one that unifies data engineering, data analytics, business intelligence (BI), and artificial intelligence (AI) within a single, scalable system.It eliminates the traditional separation between data lakes (for storage) and data warehouses (for analytics), providing a complete, end-to-end solution for modern data-driven organizations.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "End-to-End Data Management",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The lakehouse covers the entire data lifecycle ‚Äî from ingestion and storage to transformation, analysis, and advanced AI.This means all types of workloads can run on a single platform:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Batch and streaming data processing",
              "Data preparation and cleaning",
              "Business intelligence dashboards",
              "Machine learning and predictive analytics",
              "Data governance and compliance"
            ]
          },
          {
            "type": "paragraph",
            "text": "It‚Äôs not just a storage system ‚Äî it‚Äôs a comprehensive data ecosystem.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Unified Architecture",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A key part of the lakehouse‚Äôs scope is unification.It brings together multiple data roles and technologies ‚Äî data engineers, analysts, data scientists, and business teams ‚Äî under one shared platform.This allows everyone to:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Work from the same source of truth",
              "Avoid data duplication between systems",
              "Improve collaboration across departments"
            ]
          },
          {
            "type": "paragraph",
            "text": "The lakehouse bridges the gap between data lakes‚Äô flexibility and warehouses‚Äô reliability.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Scalability and Performance",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The lakehouse is designed to handle data of any size or type, from gigabytes to petabytes.It scales automatically as data grows, while maintaining high performance for both:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Analytical queries (fast, optimized SQL)",
              "AI/ML workloads (large-scale distributed computing)"
            ]
          },
          {
            "type": "paragraph",
            "text": "This makes it suitable for everything from small data projects to large enterprise analytics systems.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Governance, Security, and Compliance",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The lakehouse includes built-in capabilities for data governance ‚Äî a critical part of its scope.It provides:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Access control and permissions through systems like Unity Catalog",
              "Data lineage and audit trails for transparency",
              "Data quality management and version control",
              "Compliance with organizational and industry regulations"
            ]
          },
          {
            "type": "paragraph",
            "text": "This ensures that data remains secure, reliable, and properly managed at all stages.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Advanced Analytics and AI Integration",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Unlike traditional systems, the lakehouse is built to natively support AI and machine learning.It allows data scientists to:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Work directly on raw and curated data",
              "Build and deploy ML models with tools like MLflow",
              "Integrate with AI frameworks (TensorFlow, PyTorch, etc.)This seamless integration shortens the time from data collection to actionable insight."
            ]
          },
          {
            "type": "heading",
            "text": "Real-Time and Batch Processing",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The lakehouse supports both real-time streaming and batch workloads, giving organizations flexibility in how they process data.You can:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Stream live events for immediate analytics",
              "Schedule periodic data updates for reports and dashboardsThis dual capability broadens its scope across different use cases ‚Äî from IoT monitoring to enterprise BI."
            ]
          },
          {
            "type": "heading",
            "text": "Multi-Use Collaboration",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The platform supports collaboration between:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data Engineers ‚Äì building pipelines and managing ETL workflows",
              "Data Scientists ‚Äì training and deploying models",
              "Analysts ‚Äì running queries and creating dashboards",
              "Business Teams ‚Äì making decisions based on real-time insights"
            ]
          },
          {
            "type": "paragraph",
            "text": "All these roles can work together efficiently within the same environment.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Industry and Business Applications",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The lakehouse architecture is versatile across industries:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Finance: Fraud detection, real-time risk monitoring",
              "Retail: Personalized recommendations, demand forecasting",
              "Healthcare: Patient data analytics, predictive diagnostics",
              "Manufacturing: Supply chain optimization, IoT analytics",
              "Telecom: Customer churn prediction, network performance analysis"
            ]
          },
          {
            "type": "paragraph",
            "text": "This wide applicability is part of what makes the scope of the lakehouse so broad and transformative.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Principles for the Lakehouse",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A Lakehouse is built on a set of guiding principles that blend the flexibility of data lakes with the reliability and performance of data warehouses.These principles ensure that data can be stored, managed, and analyzed efficiently in a single, unified platform ‚Äî without the need for separate systems.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Unified Data Platform",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "At the heart of the Lakehouse is unification combining all data types and workloads in one environment.It supports structured, semi-structured, and unstructured data, allowing teams to use the same platform for:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data engineering",
              "Data analytics",
              "Machine learning and AI",
              "Business intelligence (BI)"
            ]
          },
          {
            "type": "paragraph",
            "text": "This eliminates data silos and ensures a single source of truth for all users.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Open Data Storage and Standard Formats",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Lakehouse relies on open file formats like Parquet, Delta, or ORC stored in cloud object storage (such as ADLS, S3, or GCS).This openness ensures:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Portability across platforms",
              "Easy integration with other tools and engines",
              "Avoidance of vendor lock-in",
              "Transparency and long-term accessibility"
            ]
          },
          {
            "type": "paragraph",
            "text": "In other words, your data remains yours ‚Äî always accessible and usable.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Separation of Storage and Compute",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A key architectural principle is decoupling storage from compute.This means that:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data is stored independently of the processing engines",
              "Multiple compute layers (Spark, SQL, ML, BI tools) can access the same data",
              "Scaling storage or compute can happen independently"
            ]
          },
          {
            "type": "paragraph",
            "text": "This provides flexibility, cost efficiency, and elastic scalability for different workloads.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Reliable Data Management with ACID Transactions",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Lakehouse ensures data reliability by supporting ACID (Atomicity, Consistency, Isolation, Durability) transactions ‚Äî similar to databases.This guarantees:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "No partial writes or corrupt data",
              "Consistent results across concurrent operations",
              "Reliable updates even during complex transformations"
            ]
          },
          {
            "type": "paragraph",
            "text": "Technologies like Delta Lake make this possible by maintaining transaction logs for every operation.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Unified Governance and Security",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A Lakehouse includes centralized governance, which defines how data is accessed, managed, and audited.With systems like Unity Catalog (in Databricks), you can:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Set access controls and permissions",
              "Track data lineage and versioning",
              "Ensure compliance with organizational policies",
              "Manage metadata centrally"
            ]
          },
          {
            "type": "paragraph",
            "text": "This creates a secure and well-governed environment for data collaboration.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Support for Machine Learning and AI",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Unlike traditional warehouses, the Lakehouse is AI-ready by design.It supports:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Direct access to raw and curated data for model training",
              "Integration with ML frameworks (MLflow, TensorFlow, PyTorch)",
              "Scalable compute for distributed model training",
              "Reproducibility and experiment tracking"
            ]
          },
          {
            "type": "paragraph",
            "text": "This principle bridges data analytics and data science within a single ecosystem.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "High Performance for All Workloads",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Lakehouse uses advanced techniques like:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Caching and indexing",
              "Query optimization",
              "Data skipping and column pruning"
            ]
          },
          {
            "type": "paragraph",
            "text": "These deliver warehouse-level performance while maintaining lake-level flexibility, ensuring both batch and real-time workloads run efficiently.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Versioning and Data Lineage",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Every change made to a dataset in a Lakehouse is tracked and versioned.This allows you to:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Roll back to previous versions",
              "Reproduce past experiments or reports",
              "Audit how data has evolved over time",
              "Understand data dependencies and transformations"
            ]
          },
          {
            "type": "paragraph",
            "text": "Version control enhances trust, traceability, and data quality.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Schema Enforcement and Evolution",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Lakehouse can automatically enforce and adapt schemas as data changes.It ensures:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data consistency during ingestion",
              "Schema evolution as business requirements grow",
              "Prevention of data corruption from mismatched types"
            ]
          },
          {
            "type": "paragraph",
            "text": "This principle helps maintain data integrity while still allowing flexibility.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Collaboration Across Teams",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "The Lakehouse fosters cross-functional collaboration between:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data engineers",
              "Analysts",
              "Data scientists",
              "Business stakeholders"
            ]
          },
          {
            "type": "paragraph",
            "text": "Since everyone works on the same underlying data, it reduces duplication and improves productivity.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Lakehouse architecture",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A Data Lakehouse is a modern data architecture that blends the best parts of data lakes and data warehouses.It provides the flexibility, scalability, and low cost of data lakes, while also offering the performance, data management, and reliability of data warehouses ‚Äî all within a single unified platform.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "This architecture eliminates the need to maintain separate systems for storage, analytics, and machine learning.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Key Layers of the Data Lakehouse Architecture",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "A lakehouse architecture is typically built in five core layers, each serving a distinct function but working together seamlessly.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Data Ingestion Layer",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Purpose: Collect and bring data from multiple sources into the lakehouse.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Description:This layer handles the movement of data from different systems such as:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Databases (e.g., SQL Server, Oracle, MySQL)",
              "Applications (CRM, ERP, SaaS tools)",
              "IoT devices and sensors",
              "Logs, events, and streaming sources (Kafka, Azure Event Hub)"
            ]
          },
          {
            "type": "paragraph",
            "text": "Data can arrive in batch or real-time mode.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Tools:Databricks Auto Loader, Azure Data Factory, Kafka, Apache NiFi, AWS Glue, etc.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Storage Layer",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Purpose: Store all types of data efficiently in open, scalable cloud storage.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Description:This layer serves as the foundation of the Lakehouse.It stores:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Raw data (unprocessed)",
              "Processed data",
              "Aggregated and curated data"
            ]
          },
          {
            "type": "paragraph",
            "text": "It supports structured, semi-structured, and unstructured formats (e.g., Parquet, JSON, Avro, images, videos, etc.).The data is stored in open formats for interoperability and long-term accessibility.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Tools / Technologies:Delta Lake, Apache Parquet, ORC, Cloud Object Storage (ADLS, S3, GCS).",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Metadata and Transaction Layer",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Purpose: Manage reliability, schema, and version control.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Description:This layer introduces ACID transaction support and metadata management over the data lake, turning it into a ‚ÄúLakehouse.‚ÄùIt tracks every operation ‚Äî insert, update, delete ‚Äî ensuring consistency and data integrity.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Capabilities include:",
            "heading_level": 2
          },
          {
            "type": "list",
            "items": [
              "Schema enforcement and evolution",
              "Time travel (data versioning)",
              "Data indexing for fast queries",
              "Optimized performance through caching and Z-ordering"
            ]
          },
          {
            "type": "paragraph",
            "text": "Tools:Delta Lake, Apache Iceberg, Apache Hudi.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Processing and Compute Layer",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Purpose: Transform, clean, and prepare data for analytics and machine learning.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Description:In this layer, raw data is processed and refined for various workloads:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Data cleaning and transformations",
              "Aggregations and feature engineering",
              "Real-time stream processing",
              "Batch and interactive query processing"
            ]
          },
          {
            "type": "paragraph",
            "text": "The separation of compute and storage allows scaling compute independently for efficiency.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Tools:Apache Spark, Databricks Runtime, SQL engines, PySpark, MLflow.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Serving and Consumption Layer",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Purpose: Deliver ready-to-use data for analytics, BI, and machine learning.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Description:This layer provides optimized access to curated data for:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Dashboards and visualizations (Power BI, Tableau, Looker)",
              "Ad-hoc SQL queries",
              "Data science and AI models",
              "API-driven applications"
            ]
          },
          {
            "type": "paragraph",
            "text": "Users can interact with the same underlying data, ensuring a single source of truth across teams.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Tools:Databricks SQL, Power BI, Tableau, Jupyter, MLflow, APIs.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Governance and Security Layer (Spanning All Layers)",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "Purpose: Enforce control, compliance, and data protection.",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "Description:This is a horizontal layer across all stages of the architecture. It manages:",
            "heading_level": null
          },
          {
            "type": "list",
            "items": [
              "Access control (role-based and attribute-based)",
              "Data lineage and cataloging",
              "Audit logs and compliance tracking",
              "Data masking and encryption"
            ]
          },
          {
            "type": "paragraph",
            "text": "Tools:Unity Catalog (Databricks), Purview, Ranger, AWS Lake Formation.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "How the Lakehouse Architecture Works Together",
            "heading_level": 2
          },
          {
            "type": "table",
            "rows": [
              [
                "Stage",
                "Function",
                "Output"
              ],
              [
                "Data Ingestion",
                "Collect data from multiple sources",
                "Raw data zone"
              ],
              [
                "Storage",
                "Store data in open formats",
                "Scalable data lake"
              ],
              [
                "Metadata & Transaction",
                "Add schema, ACID, and governance",
                "Reliable data layer"
              ],
              [
                "Processing",
                "Transform and prepare data",
                "Clean, curated data"
              ],
              [
                "Serving",
                "Expose data for BI, ML, AI",
                "Reports, models, insights"
              ],
              [
                "Governance",
                "Secure, audit, and manage metadata",
                "Compliance and trust"
              ]
            ]
          },
          {
            "type": "heading",
            "text": "Core Advantages of the Lakehouse Architecture",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "‚úÖ Unified platform: Combines data lake + warehouse capabilities.‚úÖ Open and flexible: Uses open file formats (no vendor lock-in).‚úÖ Reliable and consistent: ACID transactions ensure data correctness.‚úÖ Cost-efficient: Storage and compute separation reduces costs.‚úÖ Machine learning-ready: Supports AI and ML workloads natively.‚úÖ Real-time analytics: Handles both streaming and batch data.‚úÖ Governance built-in: Centralized catalog and access control.",
            "heading_level": null
          },
          {
            "type": "heading",
            "text": "Simplified View of the Lakehouse Architecture",
            "heading_level": 2
          },
          {
            "type": "paragraph",
            "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ     Data Consumers            ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ (BI Tools, AI/ML, Reports)   ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ     Serving / Query Layer     ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ  (Databricks SQL, APIs)       ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ   Processing & Compute Layer  ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ (Spark, Delta, ML Pipelines)  ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ Metadata & Transaction Layer  ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ (Delta Lake, ACID, Catalog)   ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ        Storage Layer          ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ (Parquet, JSON, Delta Files)  ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ      Data Ingestion Layer     ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îÇ (ADF, Kafka, Auto Loader)     ‚îÇ",
            "heading_level": null
          },
          {
            "type": "paragraph",
            "text": "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
            "heading_level": null
          },
          {
            "type": "table",
            "rows": [
              [
                "Aspect",
                "Traditional Data Lake",
                "Data Warehouse",
                "Data Lakehouse"
              ],
              [
                "Data types",
                "All (raw, structured, unstructured)",
                "Structured only",
                "All (with structure management)"
              ],
              [
                "Performance",
                "Low to moderate",
                "High",
                "High"
              ],
              [
                "Cost",
                "Low",
                "High",
                "Moderate"
              ],
              [
                "Reliability",
                "Low",
                "High",
                "High (with ACID)"
              ],
              [
                "ML/AI support",
                "Strong",
                "Limited",
                "Strong"
              ],
              [
                "Governance",
                "Limited",
                "Strong",
                "Strong (with Unity Catalog)"
              ],
              [
                "Scalability",
                "High",
                "Limited",
                "High"
              ]
            ]
          }
        ]
      }
    ]
  }
]